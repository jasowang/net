Bottom: bacecbccffd0e6c0a72011fccb8a4f17c2261f83
Top:    3d40ea727c14552c309467ff195d63263bc6ec94
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-03-01 21:46:42 +0800

vhost: prefetch desc indices

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index c609d5a..b8260e2 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -545,6 +545,17 @@ static bool sk_rx_array_has_data(struct sock *sk)
 		return sk_has_rx_data(sk);
 }
 
+static int sk_rx_array_length(struct sock *sk)
+{
+	struct socket *sock = sk->sk_socket;
+	struct skb_array *skb_array = tap_get_skb_array(sock->file);
+
+	if (skb_array)
+		return skb_array_peek_queue_len(skb_array);
+	else
+		return 0;
+}
+
 static int vhost_net_rx_peek_head_len(struct vhost_net *net,
 				      struct sock *sk)
 {
@@ -684,6 +695,8 @@ static void handle_rx(struct vhost_net *net)
 	struct socket *sock;
 	struct iov_iter fixup;
 	__virtio16 num_buffers;
+	__virtio16 indices[64];
+	int npkts, ndescs = 0, cur = 0;
 
 	mutex_lock(&vq->mutex);
 	sock = vq->private_data;
@@ -711,9 +724,30 @@ static void handle_rx(struct vhost_net *net)
 						&in, vq_log, &log, UIO_MAXIOV);
 		} else {
 			unsigned int out;
-			headcount = vhost_get_vq_desc(vq, vq->iov,
+			if (cur == ndescs) {
+				npkts = sk_rx_array_length(sock->sk);
+				printk("npkts %d\n", npkts);
+				if (!npkts)
+					break;
+				ndescs = vhost_prefetch_desc_indices(vq,
+								indices,
+								min(npkts, 64));
+				printk("ndescs %d\n", ndescs);
+				cur = 0;
+				if (!ndescs) {
+					headcount = 0;
+					goto enable_notify;
+				}
+				if (ndescs < 0) {
+					ndescs = 0;
+					goto out;
+				}
+			}
+
+			headcount = vhost_get_vq_desc2(vq, vq->iov,
 						ARRAY_SIZE(vq->iov),
-						&out, &in, vq_log, &log);
+						&out, &in, vq_log,
+						&log, indices[cur]);
 			if (headcount == vq->num)
 				headcount = 0;
 			else if (headcount >= 0) {
@@ -723,6 +757,7 @@ static void handle_rx(struct vhost_net *net)
 						iov_length(vq->iov, in));
 				sock_len = vhost_len - vhost_hlen;
 				headcount = 1;
+				cur++;
 			}
 		}
 
@@ -737,6 +772,7 @@ static void handle_rx(struct vhost_net *net)
 			pr_debug("Discarded rx packet: len %zd\n", sock_len);
 			continue;
 		}
+enable_notify:
 		/* OK, now we need to know about added descriptors. */
 		if (!headcount) {
 			if (unlikely(vhost_enable_notify(&net->dev, vq))) {
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 4269e62..38a9801 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -1908,6 +1908,46 @@ static int get_indirect(struct vhost_virtqueue *vq,
 	return 0;
 }
 
+/* Prefetch descriptor indices */
+int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
+				__virtio16 *indices, int num)
+{
+	int ret;
+	u16 last_avail_idx, total, left;
+	__virtio16 avail_idx;
+
+	if (unlikely(vhost_get_user(vq, avail_idx, &vq->avail->idx))) {
+		vq_err(vq, "Failed to access avail idx at %p\n",
+		       &vq->avail->idx);
+		return -EFAULT;
+	}
+	vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	ret = total = min(num, vq->avail_idx - vq->last_avail_idx);
+
+	last_avail_idx = vq->last_avail_idx & (vq->num - 1);
+	while (total) {
+		left = vq->num - last_avail_idx;
+		left = min(total, left);
+
+		ret = vhost_copy_from_user(vq, indices,
+					   &vq->avail->ring[last_avail_idx],
+					   sizeof(avail_idx) * left);
+		if (unlikely(ret)) {
+			vq_err(vq, "Failed to get descriptors\n");
+			return -EFAULT;
+		}
+
+		total -= left;
+		last_avail_idx = 0;
+	}
+
+	/* Only get avail ring entries after they have been exposed by guest. */
+	smp_rmb();
+
+	return ret;
+}
+EXPORT_SYMBOL(vhost_prefetch_desc_indices);
+
 /* This looks in the virtqueue and for the first available buffer, and converts
  * it to an iovec for convenient access.  Since descriptors consist of some
  * number of output then some number of input descriptors, it's actually two
@@ -2052,6 +2092,108 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 }
 EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 
+int vhost_get_vq_desc2(struct vhost_virtqueue *vq,
+		       struct iovec iov[], unsigned int iov_size,
+		       unsigned int *out_num, unsigned int *in_num,
+		       struct vhost_log *log, unsigned int *log_num,
+		       __virtio16 ring_head)
+{
+	struct vring_desc desc;
+	unsigned int i, head, found = 0;
+	int ret, access;
+
+	head = vhost16_to_cpu(vq, ring_head);
+
+	/* If their number is silly, that's an error. */
+	if (unlikely(head >= vq->num)) {
+		vq_err(vq, "Guest says index %u > %u is available",
+		       head, vq->num);
+		return -EINVAL;
+	}
+
+	/* When we start there are none of either input nor output. */
+	*out_num = *in_num = 0;
+	if (unlikely(log))
+		*log_num = 0;
+
+	i = head;
+	do {
+		unsigned iov_count = *in_num + *out_num;
+		if (unlikely(i >= vq->num)) {
+			vq_err(vq, "Desc index is %u > %u, head = %u",
+			       i, vq->num, head);
+			return -EINVAL;
+		}
+		if (unlikely(++found > vq->num)) {
+			vq_err(vq, "Loop detected: last one at %u "
+			       "vq size %u head %u\n",
+			       i, vq->num, head);
+			return -EINVAL;
+		}
+		ret = vhost_copy_from_user(vq, &desc, vq->desc + i,
+					   sizeof desc);
+		if (unlikely(ret)) {
+			vq_err(vq, "Failed to get descriptor: idx %d addr %p\n",
+			       i, vq->desc + i);
+			return -EFAULT;
+		}
+		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
+			ret = get_indirect(vq, iov, iov_size,
+					   out_num, in_num,
+					   log, log_num, &desc);
+			if (unlikely(ret < 0)) {
+				if (ret != -EAGAIN)
+					vq_err(vq, "Failure detected "
+						"in indirect descriptor at idx %d\n", i);
+				return ret;
+			}
+			continue;
+		}
+
+		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))
+			access = VHOST_ACCESS_WO;
+		else
+			access = VHOST_ACCESS_RO;
+		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
+				     iov_size - iov_count, access);
+		if (unlikely(ret < 0)) {
+			if (ret != -EAGAIN)
+				vq_err(vq, "Translation failure %d descriptor idx %d\n",
+					ret, i);
+			return ret;
+		}
+		if (access == VHOST_ACCESS_WO) {
+			/* If this is an input descriptor,
+			 * increment that count. */
+			*in_num += ret;
+			if (unlikely(log)) {
+				log[*log_num].addr = vhost64_to_cpu(vq, desc.addr);
+				log[*log_num].len = vhost32_to_cpu(vq, desc.len);
+				++*log_num;
+			}
+		} else {
+			/* If it's an output descriptor, they're all supposed
+			 * to come before any input descriptors. */
+			if (unlikely(*in_num)) {
+				vq_err(vq, "Descriptor has out after in: "
+				       "idx %d\n", i);
+				return -EINVAL;
+			}
+			*out_num += ret;
+		}
+	} while ((i = next_desc(vq, &desc)) != -1);
+
+	/* On success, increment avail index. */
+	vq->last_avail_idx++;
+
+	/* Assume notifications from guest are disabled at this point,
+	 * if they aren't we would need to update avail_event index. */
+	BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	return head;
+}
+EXPORT_SYMBOL_GPL(vhost_get_vq_desc2);
+
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index a9cbbb1..c8a40af 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -186,8 +186,15 @@ int vhost_get_vq_desc(struct vhost_virtqueue *,
 		      struct iovec iov[], unsigned int iov_count,
 		      unsigned int *out_num, unsigned int *in_num,
 		      struct vhost_log *log, unsigned int *log_num);
+int vhost_get_vq_desc2(struct vhost_virtqueue *,
+		       struct iovec iov[], unsigned int iov_count,
+		       unsigned int *out_num, unsigned int *in_num,
+		       struct vhost_log *log, unsigned int *log_num,
+		       __virtio16 ring_head);
 void vhost_discard_vq_desc(struct vhost_virtqueue *, int n);
 
+int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
+				__virtio16 *indices, int num);
 int vhost_vq_init_access(struct vhost_virtqueue *);
 int vhost_add_used(struct vhost_virtqueue *, unsigned int head, int len);
 int vhost_add_used_n(struct vhost_virtqueue *, struct vring_used_elem *heads,
diff --git a/include/linux/ptr_ring.h b/include/linux/ptr_ring.h
index 6c70444..73cbe89 100644
--- a/include/linux/ptr_ring.h
+++ b/include/linux/ptr_ring.h
@@ -174,6 +174,14 @@ static inline void *__ptr_ring_peek(struct ptr_ring *r)
 	return NULL;
 }
 
+static inline int __ptr_ring_peek_queue_len(struct ptr_ring *r)
+{
+	if (r->producer >= r->consumer)
+		return r->producer - r->consumer;
+	else
+		return r->producer + r->size - r->consumer;
+}
+
 /* Note: callers invoking this in a loop must use a compiler barrier,
  * for example cpu_relax(). Callers must take consumer_lock
  * if the ring is ever resized - see e.g. ptr_ring_empty.
diff --git a/include/linux/skb_array.h b/include/linux/skb_array.h
index f4dfade..92aa0de 100644
--- a/include/linux/skb_array.h
+++ b/include/linux/skb_array.h
@@ -131,6 +131,11 @@ static inline int skb_array_peek_len(struct skb_array *a)
 	return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
 }
 
+static inline int skb_array_peek_queue_len(struct skb_array *a)
+{
+	return __ptr_ring_peek_queue_len(&a->ring);
+}
+
 static inline int skb_array_peek_len_irq(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
