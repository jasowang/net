Bottom: cba26f32feba6f1d7b2a2a8acc63a3378c41f4ec
Top:    376422ea64443c41e3c5f1fe3d45d47c0fc7d567
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-09-18 15:33:47 +0800



Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 2b308e0..35bb9ae 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -20,6 +20,7 @@
 #include <linux/sched/clock.h>
 #include <linux/sched/signal.h>
 #include <linux/vmalloc.h>
+#include <linux/timer.h>
 
 #include <linux/net.h>
 #include <linux/if_packet.h>
@@ -76,6 +77,13 @@ enum {
 	VHOST_NET_VQ_MAX = 2,
 };
 
+enum {
+	VHOST_NET_BUSY_START = 0,
+	VHOST_NET_BUSY_SLEEP = 1,
+	VHOST_NET_BUSY_WAKEUP = 2,
+	VHOST_NET_BUSY_POLL = 3,
+};
+
 struct vhost_net_ubuf_ref {
 	/* refcount follows semantics similar to kref:
 	 *  0: object is released
@@ -110,6 +118,10 @@ struct vhost_net_virtqueue {
 	struct vhost_net_ubuf_ref *ubufs;
 	struct skb_array *rx_array;
 	struct vhost_net_buf rxq;
+	struct hrtimer early_timer;
+	u64 poll_ns;
+	u64 last_start;
+	int busy_state;
 };
 
 struct vhost_net {
@@ -622,8 +634,19 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned long uninitialized_var(endtime);
 	int len = peek_head_len(rvq, sk);
+	u64 end;
 
 	if (!len && vq->busyloop_timeout) {
+		nvq->last_start = ktime_get_ns();
+		if (rvq->busy_state == VHOST_NET_BUSY_POLL &&
+		    rvq->poll_ns < vq->busyloop_timeout) {
+			rvq->busy_state = VHOST_NET_BUSY_SLEEP;
+			hrtimer_start(&rvq->early_timer,
+				      rvq->poll_ns * 3 / 4,
+				      HRTIMER_MODE_REL);
+			return 0;
+		}
+
 		/* Both tx vq and rx socket were polled here */
 		mutex_lock(&vq->mutex);
 		vhost_disable_notify(&net->dev, vq);
@@ -648,6 +671,14 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 		mutex_unlock(&vq->mutex);
 
 		len = peek_head_len(rvq, sk);
+
+		rvq->poll_ns = ktime_to_ns(ktime_sub(end, start));
+	}
+
+	if (len && vq->busyloop_timeout) {
+		rvq->busy_state = VHOST_NET_BUSY_POLL;
+		end = ktime_get_ns();
+		rvq->poll_ns = ktime_to_ns(ktime_sub(end, rvq->last_start));
 	}
 
 	return len;
@@ -862,6 +893,7 @@ static void handle_rx(struct vhost_net *net)
 		}
 	}
 	vhost_net_enable_vq(net, vq);
+
 out:
 	mutex_unlock(&vq->mutex);
 }
@@ -898,6 +930,18 @@ static void handle_rx_net(struct vhost_work *work)
 	handle_rx(net);
 }
 
+static enum hrtimer_restart vhost_net_early_wakeup(struct hrtimer *timer)
+{
+	struct vhost_net_virtqueue *nvq = container_of(timer,
+						struct vhost_net_virtqueue,
+						early_timer);
+	struct vhost_virtqueue *vq = &nvq->vq;
+
+	vhost_poll_queue(&vq->poll);
+
+	return HRTIMER_NORESTART;
+}
+
 static int vhost_net_open(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n;
@@ -937,6 +981,10 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		n->vqs[i].vhost_hlen = 0;
 		n->vqs[i].sock_hlen = 0;
 		vhost_net_buf_init(&n->vqs[i].rxq);
+		hrtimer_init(&n->vqs[i].early_timer, CLOCK_MONOTONIC,
+			     HRTIMER_MODE_REL);
+		n->vqs[i].early_timer.function = vhost_net_early_wakeup;
+		n->vqs[i].early = false;
 	}
 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);
