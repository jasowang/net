Bottom: ae736c374725c2c2ef1ee2e01ba44a1dd7026a30
Top:    0c34403eb0c78698b817d2a439eb261c05897d09
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-06-13 15:23:41 +0800

Refresh of slab-early

---

diff --git a/mm/slub.c b/mm/slub.c
index 44aa784..19f9e82 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1658,6 +1658,17 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	return page;
 }
 
+static void early_gfp_check(gfp_t flags)
+{
+	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
+		gfp_t invalid_mask = flags & GFP_SLAB_BUG_MASK;
+		pr_warn("Unexpected early gfp: %#x (%pGg). Fixing up to gfp: %#x (%pGg). Fix your code!\n",
+				invalid_mask, &invalid_mask, flags, &flags);
+		dump_stack();
+	}
+
+}
+
 static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 {
 	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
@@ -2631,6 +2642,8 @@ static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	void *p;
 	unsigned long flags;
 
+	early_gfp_check(gfpflags);
+
 	local_irq_save(flags);
 #ifdef CONFIG_PREEMPT
 	/*
@@ -2664,6 +2677,8 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	struct page *page;
 	unsigned long tid;
 
+	early_gfp_check(gfpflags);
+
 	s = slab_pre_alloc_hook(s, gfpflags);
 	if (!s)
 		return NULL;
