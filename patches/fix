Bottom: 765f6fdc791f1b8f98f1a0181783191413ee08e2
Top:    3d7ca059ea5b6dd0c8915146a80c4f3267a2ed03
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-04-19 14:04:22 +0800

vhost: fix

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 7b187ec..c2ba24d 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1112,6 +1112,7 @@ static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 		if (err)
 			break;
 	} while (rq->vq->num_free);
+//	printk("rx kick\n");
 	virtqueue_kick(rq->vq);
 	return !oom;
 }
@@ -1205,6 +1206,7 @@ static int virtnet_receive(struct receive_queue *rq, int budget, bool *xdp_xmit)
 		}
 	}
 
+//	printk("num free is %d\n", rq->vq->num_free);
 	if (rq->vq->num_free > virtqueue_get_vring_size(rq->vq) / 2) {
 		if (!try_fill_recv(vi, rq, GFP_ATOMIC))
 			schedule_delayed_work(&vi->refill, 0);
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 1b3e8d2d..67b989d 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -2102,6 +2102,7 @@ EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
 	vq->last_avail_idx -= n;
+	
 }
 EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index ac4b605..0a61e4e 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -219,7 +219,7 @@ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled);
 
 #define vq_err(vq, fmt, ...) do {                                  \
-		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \
+		printk(pr_fmt(fmt), ##__VA_ARGS__);       \
 		if ((vq)->error_ctx)                               \
 				eventfd_signal((vq)->error_ctx, 1);\
 	} while (0)
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index da2a192..4ad5381 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -552,6 +552,8 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 	head = vq->next_avail_idx;
 	wrap_counter = vq->wrap_counter;
 
+//	printk("vq->indirect %d total_sg %d\n",
+//		vq->indirect, total_sg);
 	/* If the host supports indirect descriptor tables, and we have multiple
 	 * buffers, then go indirect. FIXME: tune this threshold */
 	if (vq->indirect && total_sg > 1 && vq->vq.num_free)
@@ -588,6 +590,9 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 		return -ENOSPC;
 	}
 
+//	if (_vq->index == 0)
+//		printk("chain start!\n");
+
 	for (n = 0; n < out_sgs + in_sgs; n++) {
 		for (sg = sgs[n]; sg; sg = sg_next(sg)) {
 			dma_addr_t addr = vring_map_one_sg(vq, sg, n < out_sgs ?
@@ -601,12 +606,29 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 					VRING_DESC_F_USED(!vq->wrap_counter));
 			if (!indirect && i == head)
 				head_flags = flags;
-			else
+			else {
 				desc[i].flags = flags;
+//				if (_vq->index == 0)
+//					printk("write flags to %u\n", i);
+			}
+
+#if 0
+			if (_vq->index == 0)
+				printk("vq %p desc[%d] for %s avail %d used %d\n", _vq,
+					i, n < out_sgs ? "read" : "write",
+					VRING_DESC_F_AVAIL(vq->wrap_counter),
+					VRING_DESC_F_USED(!vq->wrap_counter));
+#endif
 
 			desc[i].addr = cpu_to_virtio64(_vq->vdev, addr);
 			desc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);
 			desc[i].id = cpu_to_virtio32(_vq->vdev, head);
+
+//			if (_vq->index == 0)
+//				printk("desc[%d] id %u addr %llx len %llx\n",
+//					i, desc[i].id, desc[i].addr,
+//					desc[i].len);
+
 			prev = i;
 			i++;
 			if (!indirect && i >= vq->vring_packed.num) {
@@ -615,11 +637,17 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 			}
 		}
 	}
+
+//	if (_vq->index == 0)
+//		printk("chain end!\n");
+
 	/* Last one doesn't continue. */
 	if (total_sg == 1)
 		head_flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);
-	else
+	else {
 		desc[prev].flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);
+//		if (_vq->index == 0) printk("prev %u does not chain\n", prev);
+	}
 
 	if (indirect) {
 		/* Now that the indirect table is filled in, map it. */
@@ -664,7 +692,11 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 	 * available before all subsequent descriptors comprising
 	 * the list are made available. */
 	virtio_wmb(vq->weak_barriers);
+	smp_wmb();
 	vq->vring_packed.desc[head].flags = head_flags;
+//	if (_vq->index == 0)
+//		printk("update head %u flags next %d\n", head,
+//			head_flags &  cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT));
 	vq->num_added++;
 
 	pr_debug("Added buffer head %i to %p\n", head, vq);
@@ -871,14 +903,9 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 
 	off_wrap = virtio16_to_cpu(_vq->vdev, vq->vring_packed.device->off_wrap);
 
-	if (vq->event) {
-		// FIXME: fix this!
-		needs_kick = ((off_wrap >> 15) == vq->wrap_counter) &&
-			     vring_need_event(off_wrap & ~(1<<15), new, old);
-	} else {
-		needs_kick = (vq->vring_packed.device->flags !=
-			      cpu_to_virtio16(_vq->vdev, VRING_EVENT_F_DISABLE));
-	}
+	needs_kick = (vq->vring_packed.device->flags !=
+		      cpu_to_virtio16(_vq->vdev, VRING_EVENT_F_DISABLE));
+
 	END_USE(vq);
 	return needs_kick;
 }
@@ -1010,7 +1037,7 @@ static int detach_buf_packed(struct vring_virtqueue *vq, unsigned int head,
 	for (j = 0; j < vq->desc_state[head].num; j++) {
 		desc = &vq->vring_packed.desc[i];
 		vring_unmap_one_packed(vq, desc);
-		desc->flags = 0x0;
+//		desc->flags = 0x0;
 		i++;
 		if (i >= vq->vring_packed.num)
 			i = 0;
@@ -1029,9 +1056,9 @@ static int detach_buf_packed(struct vring_virtqueue *vq, unsigned int head,
 		len = virtio32_to_cpu(vq->vq.vdev,
 				      vq->vring_packed.desc[head].len);
 
-		BUG_ON(!(vq->vring_packed.desc[head].flags &
-			 cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_INDIRECT)));
-		BUG_ON(len == 0 || len % sizeof(struct vring_packed_desc));
+//		BUG_ON(!(vq->vring_packed.desc[head].flags &
+//			 cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_INDIRECT)));
+//		BUG_ON(len == 0 || len % sizeof(struct vring_packed_desc));
 
 		for (j = 0; j < len / sizeof(struct vring_packed_desc); j++)
 			vring_unmap_one_packed(vq, &desc[j]);
@@ -1444,6 +1471,11 @@ static bool virtqueue_enable_cb_delayed_packed(struct virtqueue *_vq)
 
 	START_USE(vq);
 
+	vq->vring_packed.driver->flags = cpu_to_virtio16(_vq->vdev,
+							 VRING_EVENT_F_ENABLE);
+	smp_mb();
+
+#if 0
 	/* We optimistically turn back on interrupts, then check if there was
 	 * more to do. */
 	/* Depending on the VIRTIO_RING_F_USED_EVENT_IDX feature, we need to
@@ -1471,6 +1503,7 @@ static bool virtqueue_enable_cb_delayed_packed(struct virtqueue *_vq)
 
 	virtio_store_mb(vq->weak_barriers, &vq->vring_packed.driver->off_wrap,
 			cpu_to_virtio16(_vq->vdev, off_wrap));
+#endif
 
 	if (more_used_packed(vq)) {
 		END_USE(vq);
@@ -1855,12 +1888,10 @@ void vring_transport_features(struct virtio_device *vdev)
 
 	for (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++) {
 		switch (i) {
-#if 0
 		case VIRTIO_RING_F_INDIRECT_DESC: // FIXME not tested yet.
 			break;
 		case VIRTIO_RING_F_EVENT_IDX: // FIXME probably not work.
 			break;
-#endif
 		case VIRTIO_F_VERSION_1:
 			break;
 		case VIRTIO_F_IOMMU_PLATFORM:
