Bottom: 3ba89040e8f9dc5c68653cb5f9debf5e07e9e470
Top:    0a21a9f1e9ad54eb32e2a6e375f1e16896b87748
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-08-31 11:09:03 +0800

vhost_net: basic batch processing of vring

XDP_DROP (without touching skb) on Intel(R) Core(TM) i7-5600U CPU @
2.60GHz (ERMS) shows:

Before: 4.25Mpps
After:  5.28Mpps (+24%)

REP GOOD:
Before: 3.01Mpps
After (no rep): 3.47Mpps
After:  3.60Mpps

Signed-off-by: Jason Wang <jasowang@redhat.com>

---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index ba08b78..71b63ef 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -447,7 +447,7 @@ static void handle_tx(struct vhost_net *net)
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned out, in;
-	int head;
+	int avails, head;
 	struct msghdr msg = {
 		.msg_name = NULL,
 		.msg_namelen = 0,
@@ -461,6 +461,7 @@ static void handle_tx(struct vhost_net *net)
 	struct socket *sock;
 	struct vhost_net_ubuf_ref *uninitialized_var(ubufs);
 	bool zcopy, zcopy_used;
+	int i;
 
 	mutex_lock(&vq->mutex);
 	sock = vq->private_data;
@@ -486,95 +487,107 @@ static void handle_tx(struct vhost_net *net)
 		if (unlikely(vhost_exceeds_maxpend(net)))
 			break;
 
-		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov,
-						ARRAY_SIZE(vq->iov),
-						&out, &in);
+		avails = vhost_prefetch_desc_indices(vq,
+						zcopy ? NULL: vq->heads,
+						VHOST_RX_BATCH);
 		/* On error, stop handling until the next kick. */
-		if (unlikely(head < 0))
+		if (unlikely(avails < 0))
 			break;
 		/* Nothing new?  Wait for eventfd to tell us they refilled. */
-		if (head == vq->num) {
+		if (!avails) {
 			if (unlikely(vhost_enable_notify(&net->dev, vq))) {
 				vhost_disable_notify(&net->dev, vq);
 				continue;
 			}
 			break;
 		}
-		if (in) {
-			vq_err(vq, "Unexpected descriptor format for TX: "
-			       "out %d, int %d\n", out, in);
-			break;
-		}
-		/* Skip header. TODO: support TSO. */
-		len = iov_length(vq->iov, out);
-		iov_iter_init(&msg.msg_iter, WRITE, vq->iov, out, len);
-		iov_iter_advance(&msg.msg_iter, hdr_size);
-		/* Sanity check */
-		if (!msg_data_left(&msg)) {
-			vq_err(vq, "Unexpected header len for TX: "
-			       "%zd expected %zd\n",
-			       len, hdr_size);
-			break;
-		}
-		len = msg_data_left(&msg);
-
-		zcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN
-				   && (nvq->upend_idx + 1) % UIO_MAXIOV !=
-				      nvq->done_idx
-				   && vhost_net_tx_select_zcopy(net);
-
-		/* use msg_control to pass vhost zerocopy ubuf info to skb */
-		if (zcopy_used) {
-			struct ubuf_info *ubuf;
-			ubuf = nvq->ubuf_info + nvq->upend_idx;
-
-			vq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);
-			vq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;
-			ubuf->callback = vhost_zerocopy_callback;
-			ubuf->ctx = nvq->ubufs;
-			ubuf->desc = nvq->upend_idx;
-			atomic_set(&ubuf->refcnt, 1);
-			msg.msg_control = ubuf;
-			msg.msg_controllen = sizeof(ubuf);
-			ubufs = nvq->ubufs;
-			atomic_inc(&ubufs->refcount);
-			nvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;
-		} else {
-			msg.msg_control = NULL;
-			ubufs = NULL;
-		}
 
-		total_len += len;
-		if (total_len < VHOST_NET_WEIGHT &&
-		    !vhost_vq_avail_empty(&net->dev, vq) &&
-		    likely(!vhost_exceeds_maxpend(net))) {
-			msg.msg_flags |= MSG_MORE;
-		} else {
-			msg.msg_flags &= ~MSG_MORE;
-		}
+		for (i = 0; i < avails; i++) {
+			head = __vhost_get_vq_desc(vq, vq->iov,
+						   ARRAY_SIZE(vq->iov),
+						   &out, &in, NULL, NULL,
+				     vhost16_to_cpu(vq, vq->heads[i].id));
+			if (in) {
+				vq_err(vq, "Unexpected descriptor format for TX: "
+				       "out %d, int %d\n", out, in);
+				goto out;
+			}
+
+			/* Skip header. TODO: support TSO. */
+			len = iov_length(vq->iov, out);
+			iov_iter_init(&msg.msg_iter, WRITE, vq->iov, out, len);
+			iov_iter_advance(&msg.msg_iter, hdr_size);
+			/* Sanity check */
+			if (!msg_data_left(&msg)) {
+				vq_err(vq, "Unexpected header len for TX: "
+					"%zd expected %zd\n",
+					len, hdr_size);
+				goto out;
+			}
+			len = msg_data_left(&msg);
 
-		/* TODO: Check specific error and bomb out unless ENOBUFS? */
-		err = sock->ops->sendmsg(sock, &msg, len);
-		if (unlikely(err < 0)) {
+			zcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN
+				     && (nvq->upend_idx + 1) % UIO_MAXIOV !=
+				         nvq->done_idx
+				     && vhost_net_tx_select_zcopy(net);
+
+			/* use msg_control to pass vhost zerocopy ubuf info to skb */
 			if (zcopy_used) {
-				vhost_net_ubuf_put(ubufs);
-				nvq->upend_idx = ((unsigned)nvq->upend_idx - 1)
-					% UIO_MAXIOV;
+				struct ubuf_info *ubuf;
+				ubuf = nvq->ubuf_info + nvq->upend_idx;
+
+				vq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);
+				vq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;
+				ubuf->callback = vhost_zerocopy_callback;
+				ubuf->ctx = nvq->ubufs;
+				ubuf->desc = nvq->upend_idx;
+				atomic_set(&ubuf->refcnt, 1);
+				msg.msg_control = ubuf;
+				msg.msg_controllen = sizeof(ubuf);
+				ubufs = nvq->ubufs;
+				atomic_inc(&ubufs->refcount);
+				nvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;
+			} else {
+				msg.msg_control = NULL;
+				ubufs = NULL;
+			}
+
+			total_len += len;
+			if (total_len < VHOST_NET_WEIGHT &&
+				!vhost_vq_avail_empty(&net->dev, vq) &&
+				likely(!vhost_exceeds_maxpend(net))) {
+				msg.msg_flags |= MSG_MORE;
+			} else {
+				msg.msg_flags &= ~MSG_MORE;
+			}
+
+			/* TODO: Check specific error and bomb out unless ENOBUFS? */
+			err = sock->ops->sendmsg(sock, &msg, len);
+			if (unlikely(err < 0)) {
+				if (zcopy_used) {
+					vhost_net_ubuf_put(ubufs);
+					nvq->upend_idx = ((unsigned)nvq->upend_idx - 1)
+						         % UIO_MAXIOV;
+				}
+				vhost_discard_vq_desc(vq, 1);
+				goto out;
+			}
+			if (err != len)
+				pr_debug("Truncated TX packet: "
+					" len %d != %zd\n", err, len);
+			if (!zcopy) {
+				vhost_add_used_idx(vq, 1);
+				vhost_signal(&net->dev, vq);
+			} else if (!zcopy_used) {
+				vhost_add_used_and_signal(&net->dev,
+							  vq, head, 0);
+			} else
+				vhost_zerocopy_signal_used(net, vq);
+			vhost_net_tx_packet(net);
+			if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
+				vhost_poll_queue(&vq->poll);
+				goto out;
 			}
-			vhost_discard_vq_desc(vq, 1);
-			break;
-		}
-		if (err != len)
-			pr_debug("Truncated TX packet: "
-				 " len %d != %zd\n", err, len);
-		if (!zcopy_used)
-			vhost_add_used_and_signal(&net->dev, vq, head, 0);
-		else
-			vhost_zerocopy_signal_used(net, vq);
-		vhost_net_tx_packet(net);
-		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
-			vhost_poll_queue(&vq->poll);
-			break;
 		}
 	}
 out:
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index a908095..155a276 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -392,7 +392,7 @@ static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)
 		vq->indirect = kmalloc(sizeof *vq->indirect * UIO_MAXIOV,
 				       GFP_KERNEL);
 		vq->log = kmalloc(sizeof *vq->log * UIO_MAXIOV, GFP_KERNEL);
-		vq->heads = kmalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);
+		vq->heads = kzalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);
 		if (!vq->indirect || !vq->log || !vq->heads)
 			goto err_nomem;
 	}
@@ -2472,7 +2472,7 @@ EXPORT_SYMBOL_GPL(vhost_dequeue_msg);
 /* Prefetch descriptor indices */
 int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
 				struct vring_used_elem *heads,
-				__virtio16 *indices, u16 num)
+				u16 num)
 {
 	int ret, ret2;
 	u16 last_avail_idx, last_used_idx, total, copied;
@@ -2485,55 +2485,40 @@ int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
 		       &vq->avail->idx);
 		return -EFAULT;
 	}
-	last_avail_idx = vq->last_avail_idx;
+	last_avail_idx = vq->last_avail_idx & (vq->num - 1);
 	vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
 	total = vq->avail_idx - vq->last_avail_idx;
 	ret = total = min(total, num);
 
-	while (total) {
-		copied = vq->num - (last_avail_idx & (vq->num - 1));
-		copied = min(copied, total);
-		ret2 = vhost_copy_from_user(vq, &indices[ret - total],
-			 &vq->avail->ring[last_avail_idx & (vq->num - 1)],
-					copied * sizeof *indices);
+	for (i = 0; i < ret; i++) {
+		ret2 = vhost_get_avail(vq, heads[i].id,
+				      &vq->avail->ring[last_avail_idx]);
 		if (unlikely(ret2)) {
 			vq_err(vq, "Failed to get descriptors\n");
 			return -EFAULT;
 		}
-
-		last_avail_idx += copied;
-		total -= copied;
-	}
-
-	if (!heads)
-		return ret;
-
-	for (i = 0; i < ret; i++) {
-		heads[i].id = indices[i];
-		heads[i].len = 0;
+		last_avail_idx = (last_avail_idx + 1) & (vq->num - 1);
 	}
 
-	total = ret;
-	last_used_idx = vq->last_used_idx;
+	last_used_idx = vq->last_used_idx & (vq->num - 1);
 	while (total) {
-		copied = vq->num - (last_used_idx & (vq->num - 1));
-		copied = min(copied, total);
+		copied = min((u16)(vq->num - last_used_idx), total);
 		ret2 = vhost_copy_to_user(vq,
-				&vq->used->ring[last_used_idx & (vq->num - 1)],
-				&heads[ret - total], copied * sizeof *used);
+					  &vq->used->ring[last_used_idx],
+					  &heads[ret - total],
+					  copied * sizeof *used);
 
 		if (unlikely(ret2)) {
 			vq_err(vq, "Failed to update used ring!\n");
 			return -EFAULT;
 		}
 
+		last_used_idx = 0;
 		total -= copied;
-		last_used_idx += copied;
 	}
 
 	/* Only get avail ring entries after they have been exposed by guest. */
 	smp_rmb();
-
 	return ret;
 }
 EXPORT_SYMBOL(vhost_prefetch_desc_indices);
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index abe7007..1943783 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -231,7 +231,7 @@ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled);
 int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
                                 struct vring_used_elem *heads,
-				__virtio16 *indices, u16 num);
+				u16 num);
 
 #define vq_err(vq, fmt, ...) do {                                  \
 		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \
diff --git a/samples/bpf/xdp1_kern.c b/samples/bpf/xdp1_kern.c
index 2197421..a4b89d3 100644
--- a/samples/bpf/xdp1_kern.c
+++ b/samples/bpf/xdp1_kern.c
@@ -51,6 +51,8 @@ int xdp_prog1(struct xdp_md *ctx)
 	u64 nh_off;
 	u32 ipproto;
 
+	return rc;
+
 	nh_off = sizeof(*eth);
 	if (data + nh_off > data_end)
 		return rc;
