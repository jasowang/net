Bottom: 1ada6eeadb0d506e78b1a8ad7d0f08443b4fe6d2
Top:    45e08d791a1860c49deffef970699045cf1f5e93
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-03-29 10:31:43 +0800

vhost-net: tx batching


---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 079cd26..eddc7b1 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -111,6 +111,7 @@ struct vhost_net_virtqueue {
 	struct ptr_ring *rx_ring;
 	struct vhost_net_buf rxq;
 	struct vring_used_elem heads[VHOST_RX_BATCH];
+	struct tun_msg_ctl ctl;
 };
 
 struct vhost_net {
@@ -475,6 +476,7 @@ static void handle_tx(struct vhost_net *net)
 	struct vhost_net_ubuf_ref *uninitialized_var(ubufs);
 	bool zcopy, zcopy_used;
 	s16 nheads = 0;
+	int n = 0, off = 0;
 
 	mutex_lock(&vq->mutex);
 	sock = vq->private_data;
@@ -491,6 +493,8 @@ static void handle_tx(struct vhost_net *net)
 	zcopy = nvq->ubufs;
 
 	for (;;) {
+		struct tun_msg *m = &nvq->ctl.msgs[nheads];
+
 		/* Release DMAs done buffers first */
 		if (zcopy)
 			vhost_zerocopy_signal_used(net, vq);
@@ -517,16 +521,16 @@ static void handle_tx(struct vhost_net *net)
 		}
 		/* Skip header. TODO: support TSO. */
 		len = iov_length(vq->iov, out);
-		iov_iter_init(&msg.msg_iter, WRITE, vq->iov, out, len);
-		iov_iter_advance(&msg.msg_iter, hdr_size);
+		iov_iter_init(m->msg_iter, WRITE, vq->iov + off, out, len);
+		iov_iter_advance(m->msg_iter, hdr_size);
 		/* Sanity check */
-		if (!msg_data_left(&msg)) {
+		if (!msg_data_left(m)) {
 			vq_err(vq, "Unexpected header len for TX: "
 			       "%zd expected %zd\n",
 			       len, hdr_size);
 			break;
 		}
-		len = msg_data_left(&msg);
+		len = msg_data_left(m);
 
 		zcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN
 				   && !vhost_exceeds_maxpend(net)
@@ -551,7 +555,8 @@ static void handle_tx(struct vhost_net *net)
 		} else {
 			nvq->heads[nheads].id = cpu_to_vhost32(vq, head);
 			nvq->heads[nheads].len = 0;
-			msg.msg_control = NULL;
+//			msg.msg_control = NULL;
+			msg->ubuf = NULL;
 			ubufs = NULL;
 		}
 
@@ -564,6 +569,31 @@ static void handle_tx(struct vhost_net *net)
 			msg.msg_flags &= ~MSG_MORE;
 		}
 
+		if (++nheads == VHOST_RX_BATCH) {
+			nvq->ctl.n = VHOST_RX_BATCH;
+			msg.msg_control = &nvq->ctl;
+			err = sock->ops->sendmsg(sock, &msg, len);
+			if (unlikely(err < 0)) {
+				vhost_discard_vq_desc(vq, VHOST_RX_BATCH);
+				vhost_net_enable_vq(net, vq);
+				break;
+			}
+			vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads,
+						    nheads);
+			nheads = 0;
+			off = 0;
+			vhost_net_tx_packet(net, VHOST_RX_BATCH);
+		} else {
+			off += out;
+		}
+
+		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
+			vhost_poll_queue(&vq->poll);
+			break;
+		}
+
+
+#if 0
 		/* TODO: Check specific error and bomb out unless ENOBUFS? */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
@@ -587,6 +617,7 @@ static void handle_tx(struct vhost_net *net)
 			nheads = 0;
 		}
 		vhost_net_tx_packet(net);
+#endif
 		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
 			vhost_poll_queue(&vq->poll);
 			break;
diff --git a/include/linux/if_tun.h b/include/linux/if_tun.h
index c5b0a75..da87d56 100644
--- a/include/linux/if_tun.h
+++ b/include/linux/if_tun.h
@@ -17,6 +17,18 @@
 
 #include <uapi/linux/if_tun.h>
 
+#define TUN_MAX_MSG 64
+
+struct tun_msg {
+	struct ubuf_info *ubuf;
+	struct iov_iter *iter;
+};
+
+struct tun_msg_ctl {
+	int n;
+	struct tun_msg msgs[TUN_MAX_MSG];
+};
+
 #define TUN_XDP_FLAG 0x1UL
 
 #if defined(CONFIG_TUN) || defined(CONFIG_TUN_MODULE)
