Bottom: 1ada6eeadb0d506e78b1a8ad7d0f08443b4fe6d2
Top:    911b92c37714f963c09759c1a1b378bcb15d0f95
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-03-29 10:31:43 +0800

vhost-net: tx batching

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index baeafa0..d816780 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -2369,16 +2369,34 @@ static void tun_sock_write_space(struct sock *sk)
 
 static int tun_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)
 {
-	int ret;
+	int ret, i;
 	struct tun_file *tfile = container_of(sock, struct tun_file, socket);
 	struct tun_struct *tun = tun_get(tfile);
+	struct tun_msg_ctl *ctl = m->msg_control;
 
 	if (!tun)
 		return -EBADFD;
 
-	ret = tun_get_user(tun, tfile, m->msg_control, &m->msg_iter,
-			   m->msg_flags & MSG_DONTWAIT,
-			   m->msg_flags & MSG_MORE);
+	if (!m) {
+		ret = tun_get_user(tun, tfile, m->msg_control, &m->msg_iter,
+				   m->msg_flags & MSG_DONTWAIT,
+				   m->msg_flags & MSG_MORE);
+		goto done;
+	}
+
+	for (i = 0; i < ctl->n; i++) {
+		struct tun_msg *msg = &ctl->msgs[i];
+
+		ret = tun_get_user(tun, tfile, msg->ubuf, &msg->iov_iter,
+				   m->msg_flags & MSG_DONTWAIT,
+				   m->msg_flags & MSG_MORE);
+		if (ret < 0)
+			goto done;
+	}
+
+	ret = 0;
+
+done:
 	tun_put(tun);
 	return ret;
 }
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 079cd26..a4d7ec3 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -111,6 +111,7 @@ struct vhost_net_virtqueue {
 	struct ptr_ring *rx_ring;
 	struct vhost_net_buf rxq;
 	struct vring_used_elem heads[VHOST_RX_BATCH];
+	struct tun_msg_ctl ctl;
 };
 
 struct vhost_net {
@@ -295,9 +296,9 @@ static void vhost_net_vq_reset(struct vhost_net *n)
 
 }
 
-static void vhost_net_tx_packet(struct vhost_net *net)
+static void vhost_net_tx_packet(struct vhost_net *net, int n)
 {
-	++net->tx_packets;
+	net->tx_packets += n;
 	if (net->tx_packets < 1024)
 		return;
 	net->tx_packets = 0;
@@ -427,7 +428,7 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				    unsigned int *out_num, unsigned int *in_num)
 {
 	unsigned long uninitialized_var(endtime);
-	int r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+	int r = vhost_get_vq_desc(vq, iov, iov_size,
 				  out_num, in_num, NULL, NULL);
 
 	if (r == vq->num && vq->busyloop_timeout) {
@@ -437,7 +438,7 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 		       vhost_vq_avail_empty(vq->dev, vq))
 			cpu_relax();
 		preempt_enable();
-		r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+		r = vhost_get_vq_desc(vq, iov, iov_size,
 				      out_num, in_num, NULL, NULL);
 	}
 
@@ -453,6 +454,29 @@ static bool vhost_exceeds_maxpend(struct vhost_net *net)
 	       min_t(unsigned int, VHOST_MAX_PEND, vq->num >> 2);
 }
 
+static int batch_tx(struct vhost_net *net,
+		    struct vhost_net_virtqueue *nvq,
+		    struct socket *sock,
+		    struct msghdr *msg, int n)
+{
+	struct vhost_virtqueue *vq = &nvq->vq;
+	int err;
+
+	nvq->ctl.n = n;
+	msg->msg_control = &nvq->ctl;
+
+	err = sock->ops->sendmsg(sock, msg, 0);
+	if (unlikely(err < 0)) {
+		vhost_discard_vq_desc(vq, n);
+		vhost_net_enable_vq(net, vq);
+		return err;
+	}
+	vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads, n);
+	vhost_net_tx_packet(net, VHOST_RX_BATCH);
+
+	return 0;
+}
+
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
 static void handle_tx(struct vhost_net *net)
@@ -474,7 +498,7 @@ static void handle_tx(struct vhost_net *net)
 	struct socket *sock;
 	struct vhost_net_ubuf_ref *uninitialized_var(ubufs);
 	bool zcopy, zcopy_used;
-	s16 nheads = 0;
+	s16 nheads = 0, off = 0;
 
 	mutex_lock(&vq->mutex);
 	sock = vq->private_data;
@@ -491,13 +515,14 @@ static void handle_tx(struct vhost_net *net)
 	zcopy = nvq->ubufs;
 
 	for (;;) {
+		struct tun_msg *m = &nvq->ctl.msgs[nheads];
+
 		/* Release DMAs done buffers first */
 		if (zcopy)
 			vhost_zerocopy_signal_used(net, vq);
 
-
-		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov,
-						ARRAY_SIZE(vq->iov),
+		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov + off,
+						ARRAY_SIZE(vq->iov) - off,
 						&out, &in);
 		/* On error, stop handling until the next kick. */
 		if (unlikely(head < 0))
@@ -516,17 +541,17 @@ static void handle_tx(struct vhost_net *net)
 			break;
 		}
 		/* Skip header. TODO: support TSO. */
-		len = iov_length(vq->iov, out);
-		iov_iter_init(&msg.msg_iter, WRITE, vq->iov, out, len);
-		iov_iter_advance(&msg.msg_iter, hdr_size);
+		len = iov_length(vq->iov + off, out);
+		iov_iter_init(&m->iov_iter, WRITE, vq->iov + off, out, len);
+		iov_iter_advance(&m->iov_iter, hdr_size);
 		/* Sanity check */
-		if (!msg_data_left(&msg)) {
+		if (!iov_iter_count(&m->iov_iter)) {
 			vq_err(vq, "Unexpected header len for TX: "
 			       "%zd expected %zd\n",
 			       len, hdr_size);
 			break;
 		}
-		len = msg_data_left(&msg);
+		len = iov_iter_count(&m->iov_iter);
 
 		zcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN
 				   && !vhost_exceeds_maxpend(net)
@@ -551,11 +576,12 @@ static void handle_tx(struct vhost_net *net)
 		} else {
 			nvq->heads[nheads].id = cpu_to_vhost32(vq, head);
 			nvq->heads[nheads].len = 0;
-			msg.msg_control = NULL;
+			m->ubuf = NULL;
 			ubufs = NULL;
 		}
 
 		total_len += len;
+#if 0
 		if (total_len < VHOST_NET_WEIGHT &&
 		    !vhost_vq_avail_empty(&net->dev, vq) &&
 		    likely(!vhost_exceeds_maxpend(net))) {
@@ -563,30 +589,20 @@ static void handle_tx(struct vhost_net *net)
 		} else {
 			msg.msg_flags &= ~MSG_MORE;
 		}
+#endif
+		msg.msg_flags &= ~MSG_MORE;
 
-		/* TODO: Check specific error and bomb out unless ENOBUFS? */
-		err = sock->ops->sendmsg(sock, &msg, len);
-		if (unlikely(err < 0)) {
-			if (zcopy_used) {
-				vhost_net_ubuf_put(ubufs);
-				nvq->upend_idx = ((unsigned)nvq->upend_idx - 1)
-					% UIO_MAXIOV;
+		if (++nheads == VHOST_RX_BATCH) {
+			err = batch_tx(net, nvq, sock, &msg, nheads);
+			if (unlikely(err < 0)) {
+				break;
 			}
-			vhost_discard_vq_desc(vq, 1);
-			vhost_net_enable_vq(net, vq);
-			break;
-		}
-		if (err != len)
-			pr_debug("Truncated TX packet: "
-				 " len %d != %zd\n", err, len);
-		if (zcopy_used)
-			vhost_zerocopy_signal_used(net, vq);
-		else if (++nheads == VHOST_RX_BATCH) {
-			vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads,
-						    nheads);
 			nheads = 0;
+			off = 0;
+		} else {
+			off += out;
 		}
-		vhost_net_tx_packet(net);
+
 		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
 			vhost_poll_queue(&vq->poll);
 			break;
@@ -594,7 +610,7 @@ static void handle_tx(struct vhost_net *net)
 	}
 out:
 	if (nheads)
-		vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads, nheads);
+		batch_tx(net, nvq, sock, &msg, nheads);
 	mutex_unlock(&vq->mutex);
 }
 
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index ac4b605..0a61e4e 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -219,7 +219,7 @@ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled);
 
 #define vq_err(vq, fmt, ...) do {                                  \
-		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \
+		printk(pr_fmt(fmt), ##__VA_ARGS__);       \
 		if ((vq)->error_ctx)                               \
 				eventfd_signal((vq)->error_ctx, 1);\
 	} while (0)
diff --git a/include/linux/if_tun.h b/include/linux/if_tun.h
index c5b0a75..809e093 100644
--- a/include/linux/if_tun.h
+++ b/include/linux/if_tun.h
@@ -17,6 +17,18 @@
 
 #include <uapi/linux/if_tun.h>
 
+#define TUN_MAX_MSG 64
+
+struct tun_msg {
+	struct ubuf_info *ubuf;
+	struct iov_iter iov_iter;
+};
+
+struct tun_msg_ctl {
+	int n;
+	struct tun_msg msgs[TUN_MAX_MSG];
+};
+
 #define TUN_XDP_FLAG 0x1UL
 
 #if defined(CONFIG_TUN) || defined(CONFIG_TUN_MODULE)
