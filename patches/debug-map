Bottom: 170d3ecb9c357d27c1d1930b316ded3d50dbafde
Top:    56b16b18d5c4a1b7771fe95098a82c5ab5be46ac
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-07-16 10:56:02 +0800

debug: map

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index b5c011a..ae508e3 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -266,6 +266,8 @@ struct virtnet_bpf_bound_prog {
 	const char *state;
 	bool is_loaded;
 	struct list_head l;
+	u32 len;
+	struct bpf_insn insnsi[0];
 };
 
 #define VIRTNET_EA(extack, msg)	NL_SET_ERR_MSG_MOD((extack), msg)
@@ -1593,12 +1595,14 @@ static bool virtnet_send_command(struct virtnet_info *vi, u8 class, u8 cmd,
 	if (unlikely(!virtqueue_kick(vi->cvq)))
 		return vi->ctrl->status == VIRTIO_NET_OK;
 
+	printk("wait for response!\n");
 	/* Spin for a response, the kick causes an ioport write, trapping
 	 * into the hypervisor, so the request should be handled immediately.
 	 */
 	while (!virtqueue_get_buf(vi->cvq, &tmp) &&
 	       !virtqueue_is_broken(vi->cvq))
 		cpu_relax();
+	printk("response get!\n");
 
 	return vi->ctrl->status == VIRTIO_NET_OK;
 }
@@ -2403,14 +2407,18 @@ static int virtnet_bpf_create_prog(struct virtnet_info *vi,
 				   struct bpf_prog *prog)
 {
 	struct virtnet_bpf_bound_prog *state;
+	size_t insn_len = prog->len * sizeof(struct bpf_insn);
 	char name[16];
 
-	state = kzalloc(sizeof(*state), GFP_KERNEL);
+	state = kzalloc(sizeof(*state) + insn_len, GFP_KERNEL);
 	if (!state)
 		return -ENOMEM;
 
+	memcpy(&state->insnsi[0], prog->insnsi, insn_len);
+
 	state->vi = vi;
 	state->prog = prog;
+	state->len = prog->len;
 	state->state = "verify";
 
 	/* Program id is not populated yet when we create the state. */
@@ -2433,19 +2441,74 @@ static int virtnet_bpf_create_prog(struct virtnet_info *vi,
 	return 0;
 }
 
+static struct virtnet_bpf_map *virtnet_get_bpf_map(struct virtnet_info *vi,
+						   struct bpf_map *map)
+{
+	struct virtnet_bpf_map *virtnet_map;
+
+	list_for_each_entry(virtnet_map, &vi->map_list, l) {
+		if (map == &virtnet_map->offmap->map)
+			return virtnet_map;
+	}
+
+	return NULL;
+}
+
 static int
 virtnet_bpf_verify_insn(struct bpf_verifier_env *env, int insn_idx,
 			int prev_insn)
 {
 	struct virtnet_bpf_bound_prog *state;
+	struct virtnet_info *vi;
+	int i;
 
 	state = env->prog->aux->offload->dev_priv;
+	vi = state->vi;
+
 	if (state->vi->bpf_bind_verifier_delay && !insn_idx)
 		msleep(state->vi->bpf_bind_verifier_delay);
 
 	if (insn_idx == env->prog->len - 1)
 		pr_vlog(env, "Hello from virtio-net!\n");
 
+	/* Replace map fd with host identitier. */
+	for (i = 0; i < state->len; i++) {
+		struct bpf_insn *insn = &state->insnsi[i];
+		struct virtnet_bpf_map *virtnet_map;
+		struct bpf_map *map;
+		struct fd f;
+
+		if (insn->code != (BPF_LD | BPF_IMM | BPF_DW))
+			continue;
+
+		printk("found map access at idx %d! fd %d\n", i, insn->imm);
+
+		f = fdget(insn->imm);
+		map = __bpf_map_get(f);
+		if (IS_ERR(map)) {
+			pr_vlog(env, "fd %d is not pointing to valid bpf_map\n",
+				insn->imm);
+			printk("fd %d is not pointing to valid bpf_map\n",
+				insn->imm);
+			return -EINVAL;
+		}
+
+		printk("find fd %d in imm\n", insn->imm);
+		virtnet_map = virtnet_get_bpf_map(vi, map);
+		if (!virtnet_map) {
+			pr_vlog(env, "could not get a offloaded map fd %d\n",
+				insn->imm);
+			printk("could not get a offloaded map fd %d\n",
+				insn->imm);
+			return -EINVAL;
+		}
+
+		printk("replace it with %d\n", virtnet_map->id);
+		insn->imm = virtnet_map->id;
+
+		fdput(f);
+	}
+
 	return 0;
 }
 
@@ -2540,6 +2603,7 @@ static int virtnet_xdp_set_prog(struct virtnet_info *vi, struct netdev_bpf *bpf)
 {
 	struct virtio_device *vdev = vi->vdev;
 	struct bpf_prog *prog = bpf->prog;
+	struct virtnet_bpf_bound_prog *bound_prog = prog->aux->offload->dev_priv;
 	struct scatterlist sg;
 	int err, i;
 
@@ -2564,13 +2628,18 @@ static int virtnet_xdp_set_prog(struct virtnet_info *vi, struct netdev_bpf *bpf)
 	printk("prog->len %d total %d\n",
 		prog->len, prog->len * sizeof(prog->insnsi[0]));
 
+	printk("bound_prog->len %d total %d\n",
+		bound_prog->len, bound_prog->len * sizeof(prog->insnsi[0]));
+
 	for (i = 0; i < prog->len; i++)
-		printk("insn %d opcode %x\n", i, prog->insnsi[i].code);
+		printk("insn %d opcode %x\n", i, bound_prog->insnsi[i].code);
+
+	memcpy(vi->ctrl->insns, bound_prog->insnsi,
+		prog->len * sizeof(bound_prog->insnsi[0]));
+	sg_init_one(&sg, vi->ctrl->insns,
+		    bound_prog->len * sizeof(bound_prog->insnsi[0]));
 
-	memcpy(vi->ctrl->insns, prog->insnsi,
-	       prog->len * sizeof(prog->insnsi[0]));
-	printk("ctl addr %p prog addr %p\n", &vi->ctrl->hdr, prog->insnsi);
-	sg_init_one(&sg, vi->ctrl->insns, prog->len * sizeof(prog->insnsi[0]));
+	printk("set offload prog!\n");
 	if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_EBPF,
 				  VIRTIO_NET_CTRL_EBPF_SET_OFFLOAD_PROG,
 				  &sg)) {
@@ -2610,8 +2679,9 @@ static int virtnet_bpf_ctrl_entry_op(struct bpf_offloaded_map *offmap,
 
 	ctrl->cmd = cpu_to_virtio64(vi->vdev, cmd);
 	ctrl->flags = cpu_to_virtio64(vi->vdev, flags);
-	ctrl->map_fd = 0; /* FIXME */
+	ctrl->map_fd = virtnet_map->id;
 
+	printk("entry op send command %d\n", ctrl->cmd);
 	sg_init_one(&sg, &vi->ctrl->ebpf, sizeof(vi->ctrl->ebpf));
 	if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_EBPF_MAP,
 				  VIRTIO_NET_CTRL_EBPF_MAP_CMD,
@@ -2713,13 +2783,14 @@ static int virtnet_bpf_map_alloc(struct virtnet_info *vi,
 
 	virtnet_map->offmap = offmap;
 	virtnet_map->id = ctrl->map_fd;
+
+	printk("success get map id from host %d\n", ctrl->map_fd);
+
 	list_add_tail(&virtnet_map->l, &vi->map_list);
 
 	return 0;
 }
 
-
-
 static int virtnet_bpf(struct net_device *dev, struct netdev_bpf *bpf)
 {
 	struct virtnet_info *vi = netdev_priv(dev);
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 074b418..b7d33a7 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -344,7 +344,6 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 			desc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT);
 			desc[i].addr = cpu_to_virtio64(_vq->vdev, addr);
 			desc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);
-			printk("out desc.addr %llx desc.len %d\n", desc[i].addr, desc[i].len);
 			prev = i;
 			i = virtio16_to_cpu(_vq->vdev, desc[i].next);
 		}
@@ -358,7 +357,6 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 			desc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT | VRING_DESC_F_WRITE);
 			desc[i].addr = cpu_to_virtio64(_vq->vdev, addr);
 			desc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);
-			printk("in desc.addr %llx desc.len %d\n", desc[i].addr, desc[i].len);
 			prev = i;
 			i = virtio16_to_cpu(_vq->vdev, desc[i].next);
 		}
diff --git a/kernel/bpf/offload.c b/kernel/bpf/offload.c
index ac747d5..7b74b2a 100644
--- a/kernel/bpf/offload.c
+++ b/kernel/bpf/offload.c
@@ -299,9 +299,11 @@ struct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr)
 
 	if (!capable(CAP_SYS_ADMIN))
 		return ERR_PTR(-EPERM);
+#if 0
 	if (attr->map_type != BPF_MAP_TYPE_ARRAY &&
 	    attr->map_type != BPF_MAP_TYPE_HASH)
 		return ERR_PTR(-EINVAL);
+#endif
 
 	offmap = kzalloc(sizeof(*offmap), GFP_USER);
 	if (!offmap)
diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0f55f79..755a5aa 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -59,6 +59,8 @@ static const struct bpf_map_ops * const bpf_map_types[] = {
 #undef BPF_MAP_TYPE
 };
 
+#define DBG() printk("file %s line %d\n", __FILE__, __LINE__)
+
 /*
  * If we're handed a bigger struct than we know of, ensure all the unknown bits
  * are 0 - i.e. new user-space does not rely on any kernel feature extensions
@@ -436,24 +438,29 @@ static int map_create(union bpf_attr *attr)
 	int f_flags;
 	int err;
 
+	DBG();
 	err = CHECK_ATTR(BPF_MAP_CREATE);
 	if (err)
 		return -EINVAL;
 
+	DBG();
 	f_flags = bpf_get_file_flag(attr->map_flags);
 	if (f_flags < 0)
 		return f_flags;
 
+	DBG();
 	if (numa_node != NUMA_NO_NODE &&
 	    ((unsigned int)numa_node >= nr_node_ids ||
 	     !node_online(numa_node)))
 		return -EINVAL;
 
+	DBG();
 	/* find map type and init map: hashtable vs rbtree vs bloom vs ... */
 	map = find_and_alloc_map(attr);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	DBG();
 	err = bpf_obj_name_cpy(map->name, attr->map_name);
 	if (err)
 		goto free_map_nouncharge;
@@ -538,6 +545,7 @@ struct bpf_map *__bpf_map_get(struct fd f)
 
 	return f.file->private_data;
 }
+EXPORT_SYMBOL(__bpf_map_get);
 
 /* prog's and map's refcnt limit */
 #define BPF_MAX_REFCNT 32768
@@ -2324,6 +2332,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 
 	switch (cmd) {
 	case BPF_MAP_CREATE:
+		DBG();
 		err = map_create(&attr);
 		break;
 	case BPF_MAP_LOOKUP_ELEM:
diff --git a/tools/lib/bpf/libbpf.c b/tools/lib/bpf/libbpf.c
index 38ed3e9..63c4830 100644
--- a/tools/lib/bpf/libbpf.c
+++ b/tools/lib/bpf/libbpf.c
@@ -930,6 +930,8 @@ bpf_program__collect_reloc(struct bpf_program *prog, GElf_Shdr *shdr,
 	size_t nr_maps = obj->nr_maps;
 	int i, nrels;
 
+	fprintf(stderr, "collect_reloc!\n");
+
 	pr_debug("collecting relocating info for: '%s'\n",
 		 prog->section_name);
 	nrels = shdr->sh_size / shdr->sh_entsize;
@@ -1006,6 +1008,8 @@ bpf_program__collect_reloc(struct bpf_program *prog, GElf_Shdr *shdr,
 			return -LIBBPF_ERRNO__RELOC;
 		}
 
+		fprintf(stderr, "relocate to mapfd insn_idx %d from %d idx %d\n",
+			(int) insn_idx, (int) insns[insn_idx].imm, (int) map_idx);
 		prog->reloc_desc[i].type = RELO_LD64;
 		prog->reloc_desc[i].insn_idx = insn_idx;
 		prog->reloc_desc[i].map_idx = map_idx;
@@ -2219,6 +2223,8 @@ int bpf_prog_load(const char *file, enum bpf_prog_type type,
 	attr.prog_type = type;
 	attr.expected_attach_type = 0;
 
+	fprintf(stderr, "load!\n");
+
 	return bpf_prog_load_xattr(&attr, pobj, prog_fd);
 }
 
@@ -2238,6 +2244,7 @@ int bpf_prog_load_xattr(const struct bpf_prog_load_attr *attr,
 	if (!attr->file)
 		return -EINVAL;
 
+	fprintf(stderr, "open !\n");
 	obj = __bpf_object__open(attr->file, NULL, 0,
 				 bpf_prog_type__needs_kver(attr->prog_type));
 	if (IS_ERR_OR_NULL(obj))
