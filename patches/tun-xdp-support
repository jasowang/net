Bottom: 2be4ba096dd79076b5a6723f7a7b3f138d85311e
Top:    f6fd3fbedf0ba42502e3c9d8a04a0fb2a3c31e64
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-07-20 15:19:27 +0800

tun: XDP support

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 5652bf3..27fb34c 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -105,7 +105,8 @@ do {								\
 } while (0)
 #endif
 
-#define TUN_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)
+#define TUN_HEADROOM 256
+#define TUN_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD + TUN_HEADROOM)
 
 /* TUN device flags */
 
@@ -173,6 +174,7 @@ struct tun_file {
 	struct tun_struct *detached;
 	struct skb_array tx_array;
 	struct page_frag alloc_frag;
+	struct bpf_prog __rcu *xdp_prog;
 };
 
 struct tun_flow_entry {
@@ -1008,6 +1010,75 @@ tun_net_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
 	stats->tx_dropped = tx_dropped;
 }
 
+static int tun_xdp_set(struct net_device *dev, struct bpf_prog *prog,
+		       struct netlink_ext_ack *extack)
+{
+	unsigned long int max_sz = PAGE_SIZE - sizeof(struct padded_vnet_hdr);
+	struct tun_struct *tun = netdev_priv(dev);
+	struct bpf_prog *old_prog;
+	struct tun_file *tfile;
+	int i, n = 0;
+
+	/* FIXME: since we want to support hybird mode, there's no
+	 * need to check for this.
+	 */
+	if (dev->mtu > max_sz) {
+		NL_SET_ERR_MSG_MOD(extack, "MTU too large to enable XDP");
+		netdev_warn(dev, "XDP requires MTU less than %lu\n", max_sz);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < tun->tun->numqueues; i++) {
+		tfile = rtnl_deference(tun->tfiles[i]);
+		old_prog = rtnl_deference(tfile->xdp_prog);
+		if (old_prog)
+			bpf_prog_put(old_prog);
+		rcu_assign_pointer(tfile->xdp_prog, prog);
+	}
+	/* FIXME: new queue prog attach */
+	list_for_each_entry(tfile, &tun->disabled, next) {
+		old_prog = rtnl_deference(tfile->xdp_prog);
+		if (old_prog)
+			bpf_prog_put(old_prog);
+		rcu_assign_pointer(tfile->xdp_prog, prog);
+		n++;
+	}
+
+	if (prog) {
+		prog = bpf_prog_add(prog, tun->numqueues + n - 1);
+		if (IS_ERR(prog))
+			return PTR_ERR(prog);
+	}
+
+}
+
+static u32 tun_xdp_query(struct net_device *dev)
+{
+	struct tun_struct *tun = netdev_priv(dev);
+	struct tun_file *tfile = rtnl_dereference(tun->tfiles[0]);
+	const struct bpf_prog *xdp_prog;
+
+	xdp_prog = rtnl_deference(tfile->xdp_prog);
+	if (xdp_prog)
+		return xdp_prog->aux->id;
+
+	return 0;
+}
+
+static int virtnet_xdp(struct net_device *dev, struct netdev_xdp *xdp)
+{
+	switch (xdp->command) {
+	case XDP_SETUP_PROG:
+		return tun_xdp_set(dev, xdp->prog, xdp->extack);
+	case XDP_QUERY_PROG:
+		xdp->prog_id = tun_xdp_query(dev);
+		xdp->prog_attached = !!xdp->prog_id;
+		return 0;
+	default:
+		return -EINVAL;
+	}
+}
+
 static const struct net_device_ops tun_netdev_ops = {
 	.ndo_uninit		= tun_net_uninit,
 	.ndo_open		= tun_net_open,
@@ -1020,6 +1091,7 @@ static const struct net_device_ops tun_netdev_ops = {
 #endif
 	.ndo_set_rx_headroom	= tun_set_headroom,
 	.ndo_get_stats64	= tun_net_get_stats64,
+	.ndo_xdp		= tun_xdp,
 };
 
 static const struct net_device_ops tap_netdev_ops = {
@@ -1201,10 +1273,12 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 {
 	struct page_frag *alloc_frag = &tfile->alloc_frag;
 	struct sk_buff *skb;
+	struct bpf_prog *xdp_prog;
 	int buflen = SKB_DATA_ALIGN(len + TUN_RX_PAD) +
 		     SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 	char *buf;
 	size_t copied;
+	bool xdp_xmit = false;
 
 	if (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))
 		return ERR_PTR(-ENOMEM);
@@ -1216,6 +1290,38 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 	if (copied != len)
 		return ERR_PTR(-EFAULT);
 
+	rcu_read_lock();
+	xdp_prog = rcu_dereference(tfile->xdp_prog);
+	if (xdp_prog) {
+		struct xpd_buff xdp;
+		void *orig_data;
+		u32 act;
+		int delta;
+
+		xdp.data_hard_start = buf;
+		xdp.data = buf + TUN_RX_PAD;
+		xdp.data_end = xdp.data + len;
+		orig_data = xdp.data;
+		act = bpf_prog_run_xdp(xdp_prog, &xdp);
+
+		switch (act) {
+		case XDP_PASS:
+			break;
+		case XDP_TX:
+			xdp_xmit = true;
+			break;
+		default:
+			bpf_warn_invalid_xdp_action(act);
+			/* fall through */
+		case XDP_ABORTED:
+			trace_xdp_exception(vi->dev, xdp_prog, act);
+			/* fall through */
+		case XDP_DROP:
+			goto err_xdp;
+		}
+	}
+	rcu_read_unlock();
+
 	skb = build_skb(buf, buflen);
 	if (!skb)
 		return ERR_PTR(-ENOMEM);
@@ -1225,7 +1331,28 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 	get_page(alloc_frag->page);
 	alloc_frag->offset += buflen;
 
+	if (xdp_xmit) {
+		int ret;
+
+		/* FIXME: vnet header */
+		skb->queue_mapping = tfile->queue_index;
+		ret = tun_net_xmit(skb, tun->dev);
+		if (ret == NET_XMIT_DROP)
+			goto err_xmit;
+
+		return NULL;
+	}
+
 	return skb;
+
+err_xdp:
+	rcu_read_unlock();
+	return NULL;
+
+err_xmit:
+	this_cpu_inc(tun->pcpu_stats->tx_dropped);
+	kfree_skb(skb);
+	return NULL;
 }
 
 /* Get packet from user space buffer */
@@ -1239,9 +1366,11 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 	size_t len = total_len, align = tun->align, linear;
 	struct virtio_net_hdr gso = { 0 };
 	struct tun_pcpu_stats *stats;
+	struct bpf_prog *xdp_prog;
 	int good_linear;
 	int copylen;
 	bool zerocopy = false;
+	bool xdp_xmit = false;
 	int err;
 	u32 rxhash;
 
@@ -1398,6 +1527,11 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 
 	tun_flow_update(tun, rxhash, tfile);
 	return total_len;
+
+err_xdp:
+	rcu_read_unlock();
+	this_cpu_inc(tun->pcpu_stats->rx_dropped);
+	return total_len;
 }
 
 static ssize_t tun_chr_write_iter(struct kiocb *iocb, struct iov_iter *from)
