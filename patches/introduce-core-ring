Bottom: aaddb235d37628d37901d074553f22b255df271b
Top:    f42f971bbb067d321669d4b963bcfb18981a9c4c
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-09-21 17:42:13 +0800

introduce core ring

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 3c9985f..8f9f6ad 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -75,6 +75,7 @@
 #include <linux/skb_array.h>
 #include <linux/bpf.h>
 #include <linux/bpf_trace.h>
+#include <linux/core_ring.h>
 
 #include <linux/uaccess.h>
 
@@ -149,6 +150,13 @@ struct tun_pcpu_stats {
 	u32 rx_frame_errors;
 };
 
+#define TUN_XDP_RING_SIZE 256
+
+struct tun_xdp_ring {
+	struct core_ring ring;
+	struct xdp_buff buffs[TUN_XDP_RING_SIZE];
+};
+
 /* A tun_file connects an open character device to a tuntap netdevice. It
  * also contains all socket related structures (except sock_fprog and tap_filter)
  * to serve as one transmit queue for tuntap device. The sock_fprog and
@@ -175,6 +183,7 @@ struct tun_file {
 	struct list_head next;
 	struct tun_struct *detached;
 	struct skb_array tx_array;
+	struct tun_xdp_ring *xdp_ring;
 };
 
 struct tun_flow_entry {
@@ -1067,6 +1076,36 @@ static const struct net_device_ops tun_netdev_ops = {
 	.ndo_get_stats64	= tun_net_get_stats64,
 };
 
+static int tun_xdp_xmit(struct net_device *dev, struct xdp_buff *xdp)
+{
+	struct tun_struct *tun = netdev_priv(dev);
+	struct tun_file *tfile = tun->tfiles[0];
+	struct core_ring *ring = &tfile->xdp_ring->ring;
+
+	if (core_ring_produce(ring, xdp)) {
+		/* Notify and wake up reader process */
+		if (tfile->flags & TUN_FASYNC)
+			kill_fasync(&tfile->fasync, SIGIO, POLL_IN);
+		tfile->socket.sk->sk_data_ready(tfile->socket.sk);
+		return -ENOSPC;
+	}
+
+	return 0;
+}
+
+static void tun_xdp_flush(struct net_device *dev)
+{
+	struct tun_struct *tun = netdev_priv(dev);
+	struct tun_file *tfile = tun->tfiles[0];
+
+	/* Notify and wake up reader process */
+	if (tfile->flags & TUN_FASYNC)
+		kill_fasync(&tfile->fasync, SIGIO, POLL_IN);
+	tfile->socket.sk->sk_data_ready(tfile->socket.sk);
+
+	return;
+}
+
 static const struct net_device_ops tap_netdev_ops = {
 	.ndo_uninit		= tun_net_uninit,
 	.ndo_open		= tun_net_open,
@@ -1084,6 +1123,8 @@ static const struct net_device_ops tap_netdev_ops = {
 	.ndo_set_rx_headroom	= tun_set_headroom,
 	.ndo_get_stats64	= tun_net_get_stats64,
 	.ndo_xdp		= tun_xdp,
+	.ndo_xdp_xmit		= tun_xdp_xmit,
+	.ndo_xdp_flush		= tun_xdp_flush,
 };
 
 static void tun_flow_init(struct tun_struct *tun)
@@ -1583,6 +1624,42 @@ static ssize_t tun_chr_write_iter(struct kiocb *iocb, struct iov_iter *from)
 	return result;
 }
 
+static ssize_t tun_put_user_xdp(struct tun_struct *tun,
+				struct tun_file *tfile,
+				struct xdp_buff *xdp,
+				struct iov_iter *iter)
+{
+	int offset, vnet_hdr_sz = 0;
+	struct page *page;
+	size_t size = xdp->data_end - xdp->data;
+	struct tun_pcpu_stats *stats;
+
+	if (tun->flags & IFF_VNET_HDR) {
+		struct virtio_net_hdr gso = { 0 };
+
+		vnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);
+		if (iov_iter_count(iter) < vnet_hdr_sz)
+			return -EINVAL;
+		if (copy_to_iter(&gso, sizeof(gso), iter) != sizeof(gso))
+			return -EFAULT;
+		iov_iter_advance(iter, vnet_hdr_sz - sizeof(gso));
+	}
+
+	page = virt_to_head_page(xdp->data);
+	offset = xdp->data - page_address(page);
+	if (copy_page_to_iter(page, offset, size, iter) != size)
+		return -EFAULT;
+
+	stats = get_cpu_ptr(tun->pcpu_stats);
+	u64_stats_update_begin(&stats->syncp);
+	stats->tx_packets++;
+	stats->tx_bytes += size;
+	u64_stats_update_end(&stats->syncp);
+	put_cpu_ptr(tun->pcpu_stats);
+
+	return 0;
+}
+
 /* Put packet to the user space buffer */
 static ssize_t tun_put_user(struct tun_struct *tun,
 			    struct tun_file *tfile,
@@ -1680,6 +1757,45 @@ static ssize_t tun_put_user(struct tun_struct *tun,
 	return total;
 }
 
+static int tun_ring_recv_xdp(struct tun_file *tfile, int noblock,
+			     struct xdp_buff *xdp)
+{
+	struct core_ring *ring = &tfile->xdp_ring->ring;
+	DECLARE_WAITQUEUE(wait, current);
+	struct xdp_buff *buff;
+	int err = 0;
+
+	buff = core_ring_consume(ring, xdp);
+	if (buff)
+		return 0;
+	if (noblock)
+		return -EAGAIN;
+
+	add_wait_queue(&tfile->wq.wait, &wait);
+	current->state = TASK_INTERRUPTIBLE;
+
+	while (1) {
+		buff = core_ring_consume(ring, xdp);
+		if (buff)
+			goto out;
+		if (signal_pending(current)) {
+			err = -ERESTARTSYS;
+			goto out;
+		}
+		if (tfile->socket.sk->sk_shutdown & RCV_SHUTDOWN) {
+			err = -EFAULT;
+			goto out;
+		}
+
+		schedule();
+	}
+
+out:
+	current->state = TASK_RUNNING;
+	remove_wait_queue(&tfile->wq.wait, &wait);
+	return err;
+}
+
 static struct sk_buff *tun_ring_recv(struct tun_file *tfile, int noblock,
 				     int *err)
 {
@@ -1726,6 +1842,7 @@ static ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,
 			   struct iov_iter *to,
 			   int noblock, struct sk_buff *skb)
 {
+	struct xdp_buff xdp;
 	ssize_t ret;
 	int err;
 
@@ -1735,6 +1852,10 @@ static ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,
 		return 0;
 
 	if (!skb) {
+		if (!tun_ring_recv_xdp(tfile, noblock, &xdp)) {
+			ret = tun_put_user_xdp(tun, tfile, &xdp, to);
+			goto xdp_out;
+		}
 		/* Read frames from ring */
 		skb = tun_ring_recv(tfile, noblock, &err);
 		if (!skb)
@@ -1748,6 +1869,10 @@ static ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,
 		consume_skb(skb);
 
 	return ret;
+
+xdp_out:
+	put_page(virt_to_head_page(xdp.data));
+	return ret;
 }
 
 static ssize_t tun_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)
@@ -2568,6 +2693,44 @@ static int tun_chr_fasync(int fd, struct file *file, int on)
 	return ret;
 }
 
+static void *tun_xdp_seek(struct core_ring *r, int i)
+{
+	struct tun_xdp_ring *ring =
+	       container_of(r, struct tun_xdp_ring, ring);
+
+	return &ring->buffs[i];
+}
+
+static bool tun_xdp_valid(struct core_ring *r, int i)
+{
+	struct tun_xdp_ring *ring =
+		container_of(r, struct tun_xdp_ring, ring);
+	struct xdp_buff *buff = &ring->buffs[i];
+
+	return buff->data != NULL;
+}
+
+static void tun_xdp_zero(struct core_ring *r, int i)
+{
+	struct tun_xdp_ring *ring =
+	       container_of(r, struct tun_xdp_ring, ring);
+	struct xdp_buff *buff = &ring->buffs[i];
+
+	buff->data = NULL;
+}
+
+static void tun_xdp_destroy(void *ptr)
+{
+}
+
+static void tun_xdp_copy(void *dst, void *src)
+{
+	struct xdp_buff *dst_buff = dst;
+	struct xdp_buff *src_buff = src;
+
+	*dst_buff = *src_buff;
+}
+
 static int tun_chr_open(struct inode *inode, struct file * file)
 {
 	struct net *net = current->nsproxy->net_ns;
@@ -2579,6 +2742,13 @@ static int tun_chr_open(struct inode *inode, struct file * file)
 					    &tun_proto, 0);
 	if (!tfile)
 		return -ENOMEM;
+
+	tfile->xdp_ring = kzalloc(sizeof *tfile->xdp_ring, GFP_KERNEL);
+	if (!tfile) {
+		sock_put(&tfile->sk);
+		return -ENOMEM;
+	}
+
 	RCU_INIT_POINTER(tfile->tun, NULL);
 	tfile->flags = 0;
 	tfile->ifindex = 0;
@@ -2599,6 +2769,11 @@ static int tun_chr_open(struct inode *inode, struct file * file)
 
 	sock_set_flag(&tfile->sk, SOCK_ZEROCOPY);
 
+	core_ring_init(&tfile->xdp_ring->ring, TUN_XDP_RING_SIZE,
+		       sizeof tfile->xdp_ring->buffs[0], GFP_KERNEL,
+		       tun_xdp_seek, tun_xdp_zero, tun_xdp_valid,
+		       tun_xdp_copy, tun_xdp_destroy);
+
 	return 0;
 }
 
@@ -2862,6 +3037,19 @@ struct skb_array *tun_get_skb_array(struct file *file)
 }
 EXPORT_SYMBOL_GPL(tun_get_skb_array);
 
+struct core_ring *tun_get_xdp_ring(struct file *file)
+{
+	struct tun_file *tfile;
+
+	if (file->f_op != &tun_fops)
+		return ERR_PTR(-EINVAL);
+	tfile = file->private_data;
+	if (!tfile)
+		return ERR_PTR(-EBADFD);
+	return &tfile->xdp_ring->ring;
+}
+EXPORT_SYMBOL_GPL(tun_get_xdp_ring);
+
 module_init(tun_init);
 module_exit(tun_cleanup);
 MODULE_DESCRIPTION(DRV_DESCRIPTION);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 58585ec..d98420f 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -394,7 +394,7 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 }
 
 static int vhost_net_enable_vq(struct vhost_net *n,
-				struct vhost_virtqueue *vq)
+			       struct vhost_virtqueue *vq)
 {
 	struct vhost_net_virtqueue *nvq =
 		container_of(vq, struct vhost_net_virtqueue, vq);
diff --git a/include/linux/core_ring.h b/include/linux/core_ring.h
new file mode 100644
index 0000000..eae3613
--- /dev/null
+++ b/include/linux/core_ring.h
@@ -0,0 +1,454 @@
+/*
+ *	Definitions for the core ring data structure.
+ *
+ *	Author:
+ *		Jason Wang <jasowang@redhat.com>
+ *
+ *	Copyright (C) 2017 Red Hat, Inc.
+ *
+ *	This program is free software; you can redistribute it and/or modify it
+ *	under the terms of the GNU General Public License as published by the
+ *	Free Software Foundation; either version 2 of the License, or (at your
+ *	option) any later version.
+ *
+ *	This is a limited-size FIFO maintaining pointers in FIFO order, with
+ *	one CPU producing entries and another consuming entries from a FIFO.
+ *
+ *	This implementation tries to minimize cache-contention when there is a
+ *	single producer and a single consumer CPU.
+ */
+
+#ifndef _LINUX_GENERIC_RING_H
+#define _LINUX_GENERIC_RING_H 1
+
+#ifdef __KERNEL__
+#include <linux/spinlock.h>
+#include <linux/cache.h>
+#include <linux/types.h>
+#include <linux/compiler.h>
+#include <linux/cache.h>
+#include <linux/slab.h>
+#include <asm/errno.h>
+#endif
+
+struct core_ring;
+
+typedef void *(*ring_seek_fn_t)(struct core_ring *r, int i);
+typedef void (*ring_zero_fn_t)(struct core_ring *r, int i);
+typedef bool (*ring_valid_fn_t)(struct core_ring *r, int i);
+typedef void (*ring_copy_fn_t)(void *dst, void *src);
+typedef void (*ring_destroy_fn_t)(void *ptr);
+
+struct core_ring {
+	int producer ____cacheline_aligned_in_smp;
+	spinlock_t producer_lock;
+	int consumer ____cacheline_aligned_in_smp;
+	spinlock_t consumer_lock;
+	/* Shared consumer/producer data */
+	/* Read-only by both the producer and the consumer */
+	int size ____cacheline_aligned_in_smp; /* max entries in queue */
+	int entry_size; /* size of entry */
+	ring_seek_fn_t seek_fn;
+	ring_zero_fn_t zero_fn;
+	ring_valid_fn_t valid_fn;
+	ring_copy_fn_t copy_fn;
+	ring_destroy_fn_t destroy_fn;
+};
+
+/* Note: callers invoking this in a loop must use a compiler barrier,
+ * for example cpu_relax().  If ring is ever resized, callers must hold
+ * producer_lock - see e.g. core_ring_full.  Otherwise, if callers don't hold
+ * producer_lock, the next call to __core_ring_produce may fail.
+ */
+static inline bool __core_ring_full(struct core_ring *r)
+{
+	return r->valid_fn(r, r->producer);
+}
+
+static inline bool core_ring_full(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock(&r->producer_lock);
+	ret = __core_ring_full(r);
+	spin_unlock(&r->producer_lock);
+
+	return ret;
+}
+
+static inline bool core_ring_full_irq(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock_irq(&r->producer_lock);
+	ret = __core_ring_full(r);
+	spin_unlock_irq(&r->producer_lock);
+
+	return ret;
+}
+
+static inline bool core_ring_full_any(struct core_ring *r)
+{
+	unsigned long flags;
+	bool ret;
+
+	spin_lock_irqsave(&r->producer_lock, flags);
+	ret = __core_ring_full(r);
+	spin_unlock_irqrestore(&r->producer_lock, flags);
+
+	return ret;
+}
+
+static inline bool core_ring_full_bh(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock_bh(&r->producer_lock);
+	ret = __core_ring_full(r);
+	spin_unlock_bh(&r->producer_lock);
+
+	return ret;
+}
+
+/* Note: callers invoking this in a loop must use a compiler barrier,
+ * for example cpu_relax(). Callers must hold producer_lock.
+ */
+static inline int __core_ring_produce(struct core_ring *r, void *ptr)
+{
+	if (unlikely(!r->size) || r->valid_fn(r, r->producer))
+		return -ENOSPC;
+
+	r->copy_fn(r->seek_fn(r, r->producer), ptr);
+	r->producer++;
+	if (unlikely(r->producer >= r->size))
+		r->producer = 0;
+	return 0;
+}
+
+/*
+ * Note: resize (below) nests producer lock within consumer lock, so if you
+ * consume in interrupt or BH context, you must disable interrupts/BH when
+ * calling this.
+ */
+static inline int core_ring_produce(struct core_ring *r, void *ptr)
+{
+	int ret;
+
+	spin_lock(&r->producer_lock);
+	ret = __core_ring_produce(r, ptr);
+	spin_unlock(&r->producer_lock);
+
+	return ret;
+}
+
+static inline int core_ring_produce_irq(struct core_ring *r, void *ptr)
+{
+	int ret;
+
+	spin_lock_irq(&r->producer_lock);
+	ret = __core_ring_produce(r, ptr);
+	spin_unlock_irq(&r->producer_lock);
+
+	return ret;
+}
+
+static inline int core_ring_produce_any(struct core_ring *r, void *ptr)
+{
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&r->producer_lock, flags);
+	ret = __core_ring_produce(r, ptr);
+	spin_unlock_irqrestore(&r->producer_lock, flags);
+
+	return ret;
+}
+
+static inline int core_ring_produce_bh(struct core_ring *r, void *ptr)
+{
+	int ret;
+
+	spin_lock_bh(&r->producer_lock);
+	ret = __core_ring_produce(r, ptr);
+	spin_unlock_bh(&r->producer_lock);
+
+	return ret;
+}
+
+/* Note: callers invoking this in a loop must use a compiler barrier,
+ * for example cpu_relax(). Callers must take consumer_lock
+ * if the ring is ever resized - see e.g. core_ring_empty.
+ */
+static inline bool __core_ring_empty(struct core_ring *r)
+{
+	return !r->valid_fn(r, r->consumer);
+}
+
+static inline bool core_ring_empty(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock(&r->consumer_lock);
+	ret = __core_ring_empty(r);
+	spin_unlock(&r->consumer_lock);
+
+	return ret;
+}
+
+static inline bool core_ring_empty_irq(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock_irq(&r->consumer_lock);
+	ret = __core_ring_empty(r);
+	spin_unlock_irq(&r->consumer_lock);
+
+	return ret;
+}
+
+static inline bool core_ring_empty_any(struct core_ring *r)
+{
+	unsigned long flags;
+	bool ret;
+
+	spin_lock_irqsave(&r->consumer_lock, flags);
+	ret = __core_ring_empty(r);
+	spin_unlock_irqrestore(&r->consumer_lock, flags);
+
+	return ret;
+}
+
+static inline bool core_ring_empty_bh(struct core_ring *r)
+{
+	bool ret;
+
+	spin_lock_bh(&r->consumer_lock);
+	ret = __core_ring_empty(r);
+	spin_unlock_bh(&r->consumer_lock);
+
+	return ret;
+}
+
+/* Note: callers invoking this in a loop must use a compiler barrier,
+ * for example cpu_relax(). Callers must take consumer_lock
+ * if they dereference the pointer - see e.g. CORE_RING_PEEK_CALL.
+ * If ring is never resized, and if the pointer is merely
+ * tested, there's no need to take the lock - see e.g.  __core_ring_empty.
+ */
+static inline void *__core_ring_peek(struct core_ring *r)
+{
+	if (likely(r->size) && !__core_ring_empty(r)) {
+		void **addr = r->seek_fn(r, r->consumer);
+		return *addr;
+	}
+	return NULL;
+}
+
+/* Must only be called after __core_ring_peek returned !NULL */
+static inline void __core_ring_discard_one(struct core_ring *r)
+{
+	r->zero_fn(r, r->consumer);
+	r->consumer++;
+	if (unlikely(r->consumer >= r->size))
+		r->consumer = 0;
+}
+
+static inline void *__core_ring_consume(struct core_ring *r, void *ptr)
+{
+	if (r->valid_fn(r, r->consumer)) {
+		r->copy_fn(ptr, r->seek_fn(r, r->consumer));
+		__core_ring_discard_one(r);
+		return ptr;
+	}
+
+	return NULL;
+}
+
+static inline int __core_ring_consume_batched(struct core_ring *r,
+					      void *array, int n)
+{
+	void *ptr;
+	int i;
+
+	for (i = 0; i < n; i++) {
+		ptr = __core_ring_consume(r, array);
+		if (!ptr)
+			break;
+		array += r->entry_size;
+	}
+
+	return i;
+}
+
+/*
+ * Note: resize (below) nests producer lock within consumer lock, so if you
+ * call this in interrupt or BH context, you must disable interrupts/BH when
+ * producing.
+ */
+static inline void *core_ring_consume(struct core_ring *r, void *ptr)
+{
+	spin_lock(&r->consumer_lock);
+	ptr = __core_ring_consume(r, ptr);
+	spin_unlock(&r->consumer_lock);
+
+	return ptr;
+}
+
+static inline void *core_ring_consume_irq(struct core_ring *r, void *ptr)
+{
+	spin_lock_irq(&r->consumer_lock);
+	ptr = __core_ring_consume(r, ptr);
+	spin_unlock_irq(&r->consumer_lock);
+
+	return ptr;
+}
+
+static inline void *core_ring_consume_any(struct core_ring *r, void *ptr)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&r->consumer_lock, flags);
+	ptr = __core_ring_consume(r, ptr);
+	spin_unlock_irqrestore(&r->consumer_lock, flags);
+
+	return ptr;
+}
+
+static inline void *core_ring_consume_bh(struct core_ring *r, void *ptr)
+{
+	spin_lock_bh(&r->consumer_lock);
+	ptr = __core_ring_consume(r, ptr);
+	spin_unlock_bh(&r->consumer_lock);
+
+	return ptr;
+}
+
+static inline int core_ring_consume_batched(struct core_ring *r,
+					    void *array, int n)
+{
+	int ret;
+
+	spin_lock(&r->consumer_lock);
+	ret = __core_ring_consume_batched(r, array, n);
+	spin_unlock(&r->consumer_lock);
+
+	return ret;
+}
+
+static inline int core_ring_consume_batched_irq(struct core_ring *r,
+					       void *array, int n)
+{
+	int ret;
+
+	spin_lock_irq(&r->consumer_lock);
+	ret = __core_ring_consume_batched(r, array, n);
+	spin_unlock_irq(&r->consumer_lock);
+
+	return ret;
+}
+
+static inline int core_ring_consume_batched_any(struct core_ring *r,
+					       void *array, int n)
+{
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&r->consumer_lock, flags);
+	ret = __core_ring_consume_batched(r, array, n);
+	spin_unlock_irqrestore(&r->consumer_lock, flags);
+
+	return ret;
+}
+
+static inline int core_ring_consume_batched_bh(struct core_ring *r,
+					      void *array, int n)
+{
+	int ret;
+
+	spin_lock_bh(&r->consumer_lock);
+	ret = __core_ring_consume_batched(r, array, n);
+	spin_unlock_bh(&r->consumer_lock);
+
+	return ret;
+}
+
+/* Cast to structure type and call a function without discarding from FIFO.
+ * Function must return a value.
+ * Callers must take consumer_lock.
+ */
+#define __CORE_RING_PEEK_CALL(r, f) ((f)(__core_ring_peek(r)))
+
+#define CORE_RING_PEEK_CALL(r, f) ({ \
+	typeof((f)(NULL)) __CORE_RING_PEEK_CALL_v; \
+	\
+	spin_lock(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v = __CORE_RING_PEEK_CALL(r, f); \
+	spin_unlock(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v; \
+})
+
+#define CORE_RING_PEEK_CALL_IRQ(r, f) ({ \
+	typeof((f)(NULL)) __CORE_RING_PEEK_CALL_v; \
+	\
+	spin_lock_irq(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v = __CORE_RING_PEEK_CALL(r, f); \
+	spin_unlock_irq(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v; \
+})
+
+#define CORE_RING_PEEK_CALL_BH(r, f) ({ \
+	typeof((f)(NULL)) __CORE_RING_PEEK_CALL_v; \
+	\
+	spin_lock_bh(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v = __CORE_RING_PEEK_CALL(r, f); \
+	spin_unlock_bh(&(r)->consumer_lock); \
+	__CORE_RING_PEEK_CALL_v; \
+})
+
+#define CORE_RING_PEEK_CALL_ANY(r, f) ({ \
+	typeof((f)(NULL)) __CORE_RING_PEEK_CALL_v; \
+	unsigned long __CORE_RING_PEEK_CALL_f;\
+	\
+	spin_lock_irqsave(&(r)->consumer_lock, __CORE_RING_PEEK_CALL_f); \
+	__CORE_RING_PEEK_CALL_v = __CORE_RING_PEEK_CALL(r, f); \
+	spin_unlock_irqrestore(&(r)->consumer_lock, __CORE_RING_PEEK_CALL_f); \
+	__CORE_RING_PEEK_CALL_v; \
+})
+
+static inline void __core_ring_set_size(struct core_ring *r, int size)
+{
+	r->size = size;
+}
+
+static inline int core_ring_init(struct core_ring *r, int size, int entry_size,
+				 gfp_t gfp,
+                                 ring_seek_fn_t seek_fn,
+				 ring_zero_fn_t zero_fn,
+				 ring_valid_fn_t valid_fn,
+				 ring_copy_fn_t copy_fn,
+				 ring_destroy_fn_t destroy_fn)
+{
+	__core_ring_set_size(r, size);
+	r->entry_size = entry_size;
+	r->producer = r->consumer = 0;
+	spin_lock_init(&r->producer_lock);
+	spin_lock_init(&r->consumer_lock);
+	r->seek_fn = seek_fn;
+	r->zero_fn = zero_fn;
+	r->valid_fn = valid_fn;
+	r->copy_fn = copy_fn;
+	r->destroy_fn = destroy_fn;
+
+	return 0;
+}
+
+static inline void core_ring_cleanup(struct core_ring *r)
+{
+	void *ptr;
+
+	if (r->destroy_fn)
+		while ((ptr = core_ring_consume(r, ptr)))
+			r->destroy_fn(ptr);
+}
+
+#endif /* _LINUX_GENERIC_RING_H  */
