Bottom: f6fd3fbedf0ba42502e3c9d8a04a0fb2a3c31e64
Top:    d51132164ff0556b0d1312f0660045577c4e84f0
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-07-25 13:42:15 +0800

Refresh of tun-xdp-support

---

diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 27fb34c..72880d2 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -73,6 +73,8 @@
 #include <linux/seq_file.h>
 #include <linux/uio.h>
 #include <linux/skb_array.h>
+#include <linux/bpf.h>
+#include <linux/bpf_trace.h>
 
 #include <linux/uaccess.h>
 
@@ -1013,31 +1015,25 @@ tun_net_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
 static int tun_xdp_set(struct net_device *dev, struct bpf_prog *prog,
 		       struct netlink_ext_ack *extack)
 {
-	unsigned long int max_sz = PAGE_SIZE - sizeof(struct padded_vnet_hdr);
 	struct tun_struct *tun = netdev_priv(dev);
 	struct bpf_prog *old_prog;
 	struct tun_file *tfile;
 	int i, n = 0;
 
-	/* FIXME: since we want to support hybird mode, there's no
-	 * need to check for this.
+	/* We will shift the packet that can't be handled to generic
+	 * XDP layer.
 	 */
-	if (dev->mtu > max_sz) {
-		NL_SET_ERR_MSG_MOD(extack, "MTU too large to enable XDP");
-		netdev_warn(dev, "XDP requires MTU less than %lu\n", max_sz);
-		return -EINVAL;
-	}
 
-	for (i = 0; i < tun->tun->numqueues; i++) {
-		tfile = rtnl_deference(tun->tfiles[i]);
-		old_prog = rtnl_deference(tfile->xdp_prog);
+	for (i = 0; i < tun->numqueues; i++) {
+		tfile = rtnl_dereference(tun->tfiles[i]);
+		old_prog = rtnl_dereference(tfile->xdp_prog);
 		if (old_prog)
 			bpf_prog_put(old_prog);
 		rcu_assign_pointer(tfile->xdp_prog, prog);
 	}
 	/* FIXME: new queue prog attach */
 	list_for_each_entry(tfile, &tun->disabled, next) {
-		old_prog = rtnl_deference(tfile->xdp_prog);
+		old_prog = rtnl_dereference(tfile->xdp_prog);
 		if (old_prog)
 			bpf_prog_put(old_prog);
 		rcu_assign_pointer(tfile->xdp_prog, prog);
@@ -1050,6 +1046,7 @@ static int tun_xdp_set(struct net_device *dev, struct bpf_prog *prog,
 			return PTR_ERR(prog);
 	}
 
+	return 0;
 }
 
 static u32 tun_xdp_query(struct net_device *dev)
@@ -1058,14 +1055,14 @@ static u32 tun_xdp_query(struct net_device *dev)
 	struct tun_file *tfile = rtnl_dereference(tun->tfiles[0]);
 	const struct bpf_prog *xdp_prog;
 
-	xdp_prog = rtnl_deference(tfile->xdp_prog);
+	xdp_prog = rtnl_dereference(tfile->xdp_prog);
 	if (xdp_prog)
 		return xdp_prog->aux->id;
 
 	return 0;
 }
 
-static int virtnet_xdp(struct net_device *dev, struct netdev_xdp *xdp)
+static int tun_xdp(struct net_device *dev, struct netdev_xdp *xdp)
 {
 	switch (xdp->command) {
 	case XDP_SETUP_PROG:
@@ -1091,7 +1088,6 @@ static const struct net_device_ops tun_netdev_ops = {
 #endif
 	.ndo_set_rx_headroom	= tun_set_headroom,
 	.ndo_get_stats64	= tun_net_get_stats64,
-	.ndo_xdp		= tun_xdp,
 };
 
 static const struct net_device_ops tap_netdev_ops = {
@@ -1110,6 +1106,7 @@ static const struct net_device_ops tap_netdev_ops = {
 	.ndo_features_check	= passthru_features_check,
 	.ndo_set_rx_headroom	= tun_set_headroom,
 	.ndo_get_stats64	= tun_net_get_stats64,
+	.ndo_xdp		= tun_xdp,
 };
 
 static void tun_flow_init(struct tun_struct *tun)
@@ -1267,7 +1264,8 @@ static void tun_rx_batched(struct tun_struct *tun, struct tun_file *tfile,
 	}
 }
 
-static struct sk_buff *tun_build_skb(struct tun_file *tfile,
+static struct sk_buff *tun_build_skb(struct tun_struct *tun,
+				     struct tun_file *tfile,
 				     struct iov_iter *from,
 				     int len)
 {
@@ -1276,6 +1274,7 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 	struct bpf_prog *xdp_prog;
 	int buflen = SKB_DATA_ALIGN(len + TUN_RX_PAD) +
 		     SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	unsigned int delta = 0;
 	char *buf;
 	size_t copied;
 	bool xdp_xmit = false;
@@ -1293,10 +1292,9 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 	rcu_read_lock();
 	xdp_prog = rcu_dereference(tfile->xdp_prog);
 	if (xdp_prog) {
-		struct xpd_buff xdp;
+		struct xdp_buff xdp;
 		void *orig_data;
 		u32 act;
-		int delta;
 
 		xdp.data_hard_start = buf;
 		xdp.data = buf + TUN_RX_PAD;
@@ -1306,15 +1304,15 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 
 		switch (act) {
 		case XDP_PASS:
-			break;
 		case XDP_TX:
+			delta = orig_data - xdp.data;
 			xdp_xmit = true;
 			break;
 		default:
 			bpf_warn_invalid_xdp_action(act);
 			/* fall through */
 		case XDP_ABORTED:
-			trace_xdp_exception(vi->dev, xdp_prog, act);
+			trace_xdp_exception(tun->dev, xdp_prog, act);
 			/* fall through */
 		case XDP_DROP:
 			goto err_xdp;
@@ -1326,15 +1324,14 @@ static struct sk_buff *tun_build_skb(struct tun_file *tfile,
 	if (!skb)
 		return ERR_PTR(-ENOMEM);
 
-	skb_reserve(skb, TUN_RX_PAD);
-	skb_put(skb, len);
+	skb_reserve(skb, TUN_RX_PAD - delta);
+	skb_put(skb, len + delta);
 	get_page(alloc_frag->page);
 	alloc_frag->offset += buflen;
 
 	if (xdp_xmit) {
 		int ret;
 
-		/* FIXME: vnet header */
 		skb->queue_mapping = tfile->queue_index;
 		ret = tun_net_xmit(skb, tun->dev);
 		if (ret == NET_XMIT_DROP)
@@ -1366,11 +1363,9 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 	size_t len = total_len, align = tun->align, linear;
 	struct virtio_net_hdr gso = { 0 };
 	struct tun_pcpu_stats *stats;
-	struct bpf_prog *xdp_prog;
 	int good_linear;
 	int copylen;
 	bool zerocopy = false;
-	bool xdp_xmit = false;
 	int err;
 	u32 rxhash;
 
@@ -1433,7 +1428,7 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 	/* FIXME: check DONTWAIT and INT_MAX */
 	if (SKB_DATA_ALIGN(len + TUN_RX_PAD) +
 	    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) < PAGE_SIZE) {
-		skb = tun_build_skb(tfile, from, len);
+		skb = tun_build_skb(tun, tfile, from, len);
 		if (IS_ERR(skb)) {
 			this_cpu_inc(tun->pcpu_stats->rx_dropped);
 			return PTR_ERR(skb);
@@ -1527,11 +1522,6 @@ static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,
 
 	tun_flow_update(tun, rxhash, tfile);
 	return total_len;
-
-err_xdp:
-	rcu_read_unlock();
-	this_cpu_inc(tun->pcpu_stats->rx_dropped);
-	return total_len;
 }
 
 static ssize_t tun_chr_write_iter(struct kiocb *iocb, struct iov_iter *from)
