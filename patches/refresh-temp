Bottom: 45e08d791a1860c49deffef970699045cf1f5e93
Top:    ec42d02c3f30bb4a52bb9ca4d0fe3d094c83dcfc
Author: Jason Wang <jasowang@redhat.com>
Date:   2018-03-29 16:40:58 +0800

Refresh of vhost-net-tx-batching

---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index eddc7b1..f11109b 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -296,9 +296,9 @@ static void vhost_net_vq_reset(struct vhost_net *n)
 
 }
 
-static void vhost_net_tx_packet(struct vhost_net *net)
+static void vhost_net_tx_packet(struct vhost_net *net, int n)
 {
-	++net->tx_packets;
+	net->tx_packets += n;
 	if (net->tx_packets < 1024)
 		return;
 	net->tx_packets = 0;
@@ -454,6 +454,29 @@ static bool vhost_exceeds_maxpend(struct vhost_net *net)
 	       min_t(unsigned int, VHOST_MAX_PEND, vq->num >> 2);
 }
 
+static int batch_tx(struct vhost_net *net,
+		    struct vhost_net_virtqueue *nvq,
+		    struct socket *sock,
+		    struct msghdr *msg, int n)
+{
+	struct vhost_virtqueue *vq = &nvq->vq;
+	int err;
+
+	nvq->ctl.n = n;
+	msg->msg_control = &nvq->ctl;
+
+	err = sock->ops->sendmsg(sock, msg, 0);
+	if (unlikely(err < 0)) {
+		vhost_discard_vq_desc(vq, VHOST_RX_BATCH);
+		vhost_net_enable_vq(net, vq);
+		return err;
+	}
+	vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads, n);
+	vhost_net_tx_packet(net, VHOST_RX_BATCH);
+
+	return 0;
+}
+
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
 static void handle_tx(struct vhost_net *net)
@@ -475,8 +498,7 @@ static void handle_tx(struct vhost_net *net)
 	struct socket *sock;
 	struct vhost_net_ubuf_ref *uninitialized_var(ubufs);
 	bool zcopy, zcopy_used;
-	s16 nheads = 0;
-	int n = 0, off = 0;
+	s16 nheads = 0, off = 0;
 
 	mutex_lock(&vq->mutex);
 	sock = vq->private_data;
@@ -500,8 +522,8 @@ static void handle_tx(struct vhost_net *net)
 			vhost_zerocopy_signal_used(net, vq);
 
 
-		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov,
-						ARRAY_SIZE(vq->iov),
+		head = vhost_net_tx_get_vq_desc(net, vq, vq->iov + off,
+						ARRAY_SIZE(vq->iov) - off,
 						&out, &in);
 		/* On error, stop handling until the next kick. */
 		if (unlikely(head < 0))
@@ -520,17 +542,17 @@ static void handle_tx(struct vhost_net *net)
 			break;
 		}
 		/* Skip header. TODO: support TSO. */
-		len = iov_length(vq->iov, out);
-		iov_iter_init(m->msg_iter, WRITE, vq->iov + off, out, len);
-		iov_iter_advance(m->msg_iter, hdr_size);
+		len = iov_length(vq->iov + off, out);
+		iov_iter_init(&m->iov_iter, WRITE, vq->iov + off, out, len);
+		iov_iter_advance(&m->iov_iter, hdr_size);
 		/* Sanity check */
-		if (!msg_data_left(m)) {
+		if (!iov_iter_count(&m->iov_iter)) {
 			vq_err(vq, "Unexpected header len for TX: "
 			       "%zd expected %zd\n",
 			       len, hdr_size);
 			break;
 		}
-		len = msg_data_left(m);
+		len = iov_iter_count(&m->iov_iter);
 
 		zcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN
 				   && !vhost_exceeds_maxpend(net)
@@ -555,8 +577,7 @@ static void handle_tx(struct vhost_net *net)
 		} else {
 			nvq->heads[nheads].id = cpu_to_vhost32(vq, head);
 			nvq->heads[nheads].len = 0;
-//			msg.msg_control = NULL;
-			msg->ubuf = NULL;
+			m->ubuf = NULL;
 			ubufs = NULL;
 		}
 
@@ -570,19 +591,11 @@ static void handle_tx(struct vhost_net *net)
 		}
 
 		if (++nheads == VHOST_RX_BATCH) {
-			nvq->ctl.n = VHOST_RX_BATCH;
-			msg.msg_control = &nvq->ctl;
-			err = sock->ops->sendmsg(sock, &msg, len);
-			if (unlikely(err < 0)) {
-				vhost_discard_vq_desc(vq, VHOST_RX_BATCH);
-				vhost_net_enable_vq(net, vq);
+			err = batch_tx(net, nvq, sock, &msg, nheads);
+			if (unlikely(err < 0))
 				break;
-			}
-			vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads,
-						    nheads);
 			nheads = 0;
 			off = 0;
-			vhost_net_tx_packet(net, VHOST_RX_BATCH);
 		} else {
 			off += out;
 		}
@@ -591,41 +604,10 @@ static void handle_tx(struct vhost_net *net)
 			vhost_poll_queue(&vq->poll);
 			break;
 		}
-
-
-#if 0
-		/* TODO: Check specific error and bomb out unless ENOBUFS? */
-		err = sock->ops->sendmsg(sock, &msg, len);
-		if (unlikely(err < 0)) {
-			if (zcopy_used) {
-				vhost_net_ubuf_put(ubufs);
-				nvq->upend_idx = ((unsigned)nvq->upend_idx - 1)
-					% UIO_MAXIOV;
-			}
-			vhost_discard_vq_desc(vq, 1);
-			vhost_net_enable_vq(net, vq);
-			break;
-		}
-		if (err != len)
-			pr_debug("Truncated TX packet: "
-				 " len %d != %zd\n", err, len);
-		if (zcopy_used)
-			vhost_zerocopy_signal_used(net, vq);
-		else if (++nheads == VHOST_RX_BATCH) {
-			vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads,
-						    nheads);
-			nheads = 0;
-		}
-		vhost_net_tx_packet(net);
-#endif
-		if (unlikely(total_len >= VHOST_NET_WEIGHT)) {
-			vhost_poll_queue(&vq->poll);
-			break;
-		}
 	}
 out:
 	if (nheads)
-		vhost_add_used_and_signal_n(&net->dev, vq, nvq->heads, nheads);
+		batch_tx(net, nvq, sock, &msg, nheads);
 	mutex_unlock(&vq->mutex);
 }
 
diff --git a/include/linux/if_tun.h b/include/linux/if_tun.h
index da87d56..809e093 100644
--- a/include/linux/if_tun.h
+++ b/include/linux/if_tun.h
@@ -21,7 +21,7 @@
 
 struct tun_msg {
 	struct ubuf_info *ubuf;
-	struct iov_iter *iter;
+	struct iov_iter iov_iter;
 };
 
 struct tun_msg_ctl {
