Bottom: 148c5ed3a5a3e04b556ca70b074c29470aa7d9e9
Top:    d98c67aadaf92e3579d157f80e55337d8c700e5f
Author: Jason Wang <jasowang@redhat.com>
Date:   2017-03-22 13:21:46 +0800

vhost_net: prefetch desc indices and used ring batched updating

Signed-off-by: Jason Wang <jasowang@redhat.com>


---

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 41153a3..0ef4f3f 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -675,6 +675,40 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	return r;
 }
 
+static void handle_rx_batched(struct vhost_net *net)
+{
+	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];
+	struct vhost_virtqueue *vq = &nvq->vq;
+	__virtio16 indices[VHOST_RX_BATCHED];
+	int sock_len, i;
+	int avails, head;
+
+	while ((sock_len = vhost_net_rx_peek_head_len(net, sock->sk))) {
+retry:
+		avails = vhost_prefetch_desc_indices(vq, indices,
+						     nvq->rt - nvq->rh);
+		if (!avail) {
+			if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+				/* They have slipped one in as we were
+				 * doing that: check again. */
+				vhost_disable_notify(&net->dev, vq);
+				goto retry;
+			}
+			goto out;
+		}
+		for (i = 0; i < avails; i++) {
+			int len =__skb_array_len_with_tag(rvq->rxq[rvq->rh + i]);
+			
+			vhost_add_used(indices[i], cpu_to_vhost32(vq, len));
+		}
+
+		/* Make sure buffer is written before we update index. */
+		smp_wmb();
+
+		
+	}
+}
+
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
 static void handle_rx(struct vhost_net *net)
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index f0ba362..2ab90a9 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -1994,6 +1994,44 @@ static int get_indirect(struct vhost_virtqueue *vq,
 	return 0;
 }
 
+/* Prefetch descriptor indices */
+int vhost_prefetch_desc_indices(struct vhost_virtqueue *vq,
+				struct vring_used_elem_heads *heads, u16 num)
+{
+	int ret = 0;
+	u16 last_avail_idx, total;
+	__virtio16 avail_idx;
+
+	if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
+		vq_err(vq, "Failed to access avail idx at %p\n",
+		       &vq->avail->idx);
+		return -EFAULT;
+	}
+	last_avail_idx = vq->last_avail_idx;
+	vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	total = vq->avail_idx - vq->last_avail_idx;
+	ret = total = min(total, num);
+
+	while (total) {
+		int ret2 = vhost_get_avail(vq, avail_idx,
+			   &vq->avail->ring[last_avail_idx & (vq->num - 1)]);
+		if (unlikely(ret2)) {
+			vq_err(vq, "Failed to get descriptors\n");
+			return -EFAULT;
+		}
+		--total;
+		heads[indices++].id = vhost16_to_cpu(vq, avail_idx);
+		++last_avail_idx;
+	}
+	/* FIXME: update used ring here? together with batch dequing? */
+
+	/* Only get avail ring entries after they have been exposed by guest. */
+	smp_rmb();
+
+	return ret;
+}
+EXPORT_SYMBOL(vhost_prefetch_desc_indices);
+
 /* This looks in the virtqueue and for the first available buffer, and converts
  * it to an iovec for convenient access.  Since descriptors consist of some
  * number of output then some number of input descriptors, it's actually two
